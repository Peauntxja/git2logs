#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GitLab æäº¤æ—¥å¿—ç”Ÿæˆå·¥å…·
ä» GitLab ä»“åº“è·å–æŒ‡å®šæäº¤è€…æ¯å¤©çš„ä»£ç æäº¤ï¼Œç”Ÿæˆç®€æ´çš„ Markdown æ ¼å¼æ—¥å¿—
"""
import argparse
import sys
import os
from datetime import datetime
from collections import defaultdict
from urllib.parse import urlparse
import logging
import statistics

try:
    import gitlab  # pyright: ignore[reportMissingImports]
except ImportError:
    print("é”™è¯¯: æœªå®‰è£… python-gitlab åº“")
    print("è¯·è¿è¡Œ: pip install python-gitlab")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def create_gitlab_client(gitlab_url, token=None):
    """
    åˆ›å»º GitLab å®¢æˆ·ç«¯è¿æ¥
    
    Args:
        gitlab_url: GitLab å®ä¾‹ URLï¼ˆä¾‹å¦‚ï¼šhttps://gitlab.comï¼‰
        token: è®¿é—®ä»¤ç‰Œï¼ˆå¯é€‰ï¼Œç§æœ‰ä»“åº“éœ€è¦ï¼‰
    
    Returns:
        gitlab.Gitlab: GitLab å®¢æˆ·ç«¯å®ä¾‹
    """
    if not token:
        logger.warning("æœªæä¾›è®¿é—®ä»¤ç‰Œï¼Œå¯èƒ½æ— æ³•è®¿é—®ç§æœ‰ä»“åº“")
    
    try:
        gl = gitlab.Gitlab(gitlab_url, private_token=token)
        gl.auth()  # éªŒè¯è¿æ¥
        logger.info(f"æˆåŠŸè¿æ¥åˆ° GitLab å®ä¾‹: {gitlab_url}")
        return gl
    except Exception as e:
        logger.error(f"è¿æ¥ GitLab å¤±è´¥: {str(e)}")
        raise


def parse_project_identifier(repo_url):
    """
    ä»ä»“åº“ URL æˆ–è·¯å¾„è§£æé¡¹ç›®æ ‡è¯†ç¬¦
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - https://gitlab.com/group/project
    - https://gitlab.com/group/project.git
    - http://gitlab.example.com/group/project.git
    - group/project
    - group%2Fproject
    
    Args:
        repo_url: ä»“åº“ URL æˆ–è·¯å¾„
    
    Returns:
        str: é¡¹ç›®æ ‡è¯†ç¬¦ï¼ˆgroup/project æ ¼å¼ï¼‰
    """
    # å¦‚æœæ˜¯å®Œæ•´çš„ URL
    if repo_url.startswith('http://') or repo_url.startswith('https://'):
        parsed = urlparse(repo_url)
        path = parsed.path.strip('/')
        # ç§»é™¤ .git åç¼€
        if path.endswith('.git'):
            path = path[:-4]
        return path
    else:
        # ç›´æ¥æ˜¯è·¯å¾„æ ¼å¼
        return repo_url.strip('/')


def extract_gitlab_url(repo_url):
    """
    ä»ä»“åº“ URL ä¸­æå– GitLab å®ä¾‹ URL
    
    Args:
        repo_url: ä»“åº“ URL
    
    Returns:
        str: GitLab å®ä¾‹ URLï¼Œå¦‚æœä¸æ˜¯å®Œæ•´ URL åˆ™è¿”å› None
    """
    if repo_url.startswith('http://') or repo_url.startswith('https://'):
        parsed = urlparse(repo_url)
        return f"{parsed.scheme}://{parsed.netloc}"
    return None


def get_commits_by_author(project, author_name, since_date=None, until_date=None, branch=None):
    """
    è·å–æŒ‡å®šæäº¤è€…çš„æ‰€æœ‰æäº¤
    
    Args:
        project: GitLab é¡¹ç›®å¯¹è±¡
        author_name: æäº¤è€…å§“åæˆ–é‚®ç®±
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
        branch: åˆ†æ”¯åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤æŸ¥è¯¢æ‰€æœ‰åˆ†æ”¯ï¼‰
    
    Returns:
        list: æäº¤åˆ—è¡¨
    """
    if branch:
        logger.info(f"å¼€å§‹è·å–æäº¤è€… '{author_name}' åœ¨åˆ†æ”¯ '{branch}' çš„æäº¤è®°å½•...")
    else:
        logger.info(f"å¼€å§‹è·å–æäº¤è€… '{author_name}' çš„æäº¤è®°å½•...")
    
    commits = []
    page = 1
    per_page = 100
    
    # å¦‚æœæŒ‡å®šäº†åˆ†æ”¯ï¼Œç›´æ¥æŸ¥è¯¢è¯¥åˆ†æ”¯
    if branch:
        params = {
            'author': author_name,
            'ref_name': branch,
            'per_page': per_page
        }
        
        # æ·»åŠ æ—¥æœŸèŒƒå›´ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if since_date:
            params['since'] = f"{since_date}T00:00:00Z"
        if until_date:
            params['until'] = f"{until_date}T23:59:59Z"
        
        logger.debug(f"æŸ¥è¯¢å‚æ•°: author={author_name}, since={params.get('since')}, until={params.get('until')}, branch={branch}")
        
        try:
            # é¦–å…ˆå°è¯•ä¸æŒ‡å®šä½œè€…ï¼Œè·å–ä¸€äº›æäº¤çœ‹çœ‹å®é™…çš„ä½œè€…æ ¼å¼
            try:
                debug_params = {'ref_name': branch, 'per_page': 5}
                if since_date:
                    debug_params['since'] = f"{since_date}T00:00:00Z"
                if until_date:
                    debug_params['until'] = f"{until_date}T23:59:59Z"
                debug_commits = project.commits.list(**debug_params)
                if debug_commits:
                    logger.info("è°ƒè¯•ï¼šæŸ¥è¯¢åˆ°çš„æäº¤ç¤ºä¾‹ï¼ˆä¸æŒ‡å®šä½œè€…ï¼‰ï¼š")
                    for idx, dc in enumerate(debug_commits[:3], 1):
                        dc_author = getattr(dc, 'author_name', 'N/A')
                        dc_email = getattr(dc, 'author_email', 'N/A')
                        logger.info(f"  æäº¤ {idx}: ä½œè€…='{dc_author}' é‚®ç®±='{dc_email}'")
            except Exception as e:
                logger.debug(f"è°ƒè¯•æŸ¥è¯¢å¤±è´¥: {e}")
            
            while True:
                params['page'] = page
                page_commits = project.commits.list(**params)
                
                if not page_commits:
                    # å¦‚æœç¬¬ä¸€é¡µå°±æ²¡æœ‰ç»“æœï¼Œå°è¯•ä¸åŒçš„ author æ ¼å¼
                    if page == 1:
                        import re
                        # å°è¯•æå–é‚®ç®±ï¼ˆå¦‚æœæ ¼å¼æ˜¯ "Name <email>"ï¼‰
                        email_match = re.search(r'<([^>]+)>', author_name)
                        if email_match:
                            email_only = email_match.group(1)
                            logger.info(f"å°è¯•ä½¿ç”¨é‚®ç®±æ ¼å¼æŸ¥è¯¢: {email_only}")
                            params_alt = params.copy()
                            params_alt['author'] = email_only
                            try:
                                page_commits_alt = project.commits.list(**params_alt)
                                if page_commits_alt:
                                    logger.info(f"âœ“ ä½¿ç”¨é‚®ç®±æ ¼å¼æ‰¾åˆ° {len(page_commits_alt)} æ¡æäº¤")
                                    page_commits = page_commits_alt
                                    params = params_alt
                                    # æ‰¾åˆ°æäº¤åï¼Œç»§ç»­å¤„ç†ï¼Œä¸è¦ break
                                else:
                                    logger.info(f"âœ— ä½¿ç”¨é‚®ç®±æ ¼å¼æœªæ‰¾åˆ°æäº¤")
                            except Exception as e:
                                logger.debug(f"ä½¿ç”¨é‚®ç®±æ ¼å¼æŸ¥è¯¢å¤±è´¥: {e}")
                        
                        # å¦‚æœé‚®ç®±æ ¼å¼æ²¡æ‰¾åˆ°ï¼Œå°è¯•åªä½¿ç”¨åç§°éƒ¨åˆ†ï¼ˆå¦‚æœæ ¼å¼æ˜¯ "Name <email>"ï¼‰
                        if not page_commits:
                            name_match = re.match(r'^([^<]+)', author_name)
                            if name_match:
                                name_only = name_match.group(1).strip()
                                if name_only and name_only != author_name:
                                    logger.info(f"å°è¯•ä½¿ç”¨åç§°æ ¼å¼æŸ¥è¯¢: '{name_only}'")
                                    params_alt = params.copy()
                                    params_alt['author'] = name_only
                                    try:
                                        page_commits_alt = project.commits.list(**params_alt)
                                        if page_commits_alt:
                                            logger.info(f"âœ“ ä½¿ç”¨åç§°æ ¼å¼æ‰¾åˆ° {len(page_commits_alt)} æ¡æäº¤")
                                            page_commits = page_commits_alt
                                            params = params_alt
                                            # æ‰¾åˆ°æäº¤åï¼Œç»§ç»­å¤„ç†ï¼Œä¸è¦ break
                                        else:
                                            logger.info(f"âœ— ä½¿ç”¨åç§°æ ¼å¼æœªæ‰¾åˆ°æäº¤")
                                    except Exception as e:
                                        logger.debug(f"ä½¿ç”¨åç§°æ ¼å¼æŸ¥è¯¢å¤±è´¥: {e}")
                        
                        # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œç»™å‡ºæç¤ºå¹¶é€€å‡º
                        if not page_commits:
                            logger.warning("æ‰€æœ‰ä½œè€…æ ¼å¼éƒ½æœªæ‰¾åˆ°æäº¤ï¼Œå¯èƒ½çš„åŸå› ï¼š")
                            logger.warning("1. è¯¥åˆ†æ”¯åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…ç¡®å®æ²¡æœ‰æäº¤")
                            logger.warning("2. ä½œè€…åç§°æ ¼å¼ä¸åŒ¹é…ï¼ˆè¯·æ£€æŸ¥ä¸Šé¢çš„ç¤ºä¾‹æäº¤ä½œè€…æ ¼å¼ï¼‰")
                            logger.warning("3. æ—¥æœŸèŒƒå›´é—®é¢˜ï¼ˆGitLab ä½¿ç”¨ UTC æ—¶é—´ï¼‰")
                            break
                    else:
                        # ä¸æ˜¯ç¬¬ä¸€é¡µï¼Œæ²¡æœ‰æ›´å¤šç»“æœï¼Œé€€å‡º
                        break
                
                # å¤„ç†æ‰¾åˆ°çš„æäº¤
                if page_commits:
                    commits.extend(page_commits)
                    logger.info(f"å·²è·å– {len(commits)} æ¡æäº¤è®°å½•...")
                    
                    if len(page_commits) < per_page:
                        break
                    
                    page += 1
                else:
                    break
            
            logger.info(f"å…±è·å–åˆ° {len(commits)} æ¡æäº¤è®°å½•")
            return commits
        except Exception as e:
            logger.error(f"è·å–æäº¤è®°å½•å¤±è´¥: {str(e)}")
            raise
    else:
        # ä¸æŒ‡å®šåˆ†æ”¯æ—¶ï¼Œéå†æ‰€æœ‰åˆ†æ”¯æŸ¥è¯¢
        # GitLab API çš„ all=True å‚æ•°å¯èƒ½æ— æ³•æ­£ç¡®æŒ‰ä½œè€…è¿‡æ»¤
        logger.info("æœªæŒ‡å®šåˆ†æ”¯ï¼Œå°†éå†æ‰€æœ‰åˆ†æ”¯æŸ¥è¯¢...")
        all_commits = []
        branches = project.branches.list(per_page=100)
        logger.info(f"æ‰¾åˆ° {len(branches)} ä¸ªåˆ†æ”¯ï¼Œå¼€å§‹éå†æŸ¥è¯¢...")
        
        for idx, branch_obj in enumerate(branches, 1):
            try:
                branch_params = {
                    'author': author_name,
                    'ref_name': branch_obj.name,
                    'per_page': per_page
                }
                
                if since_date:
                    branch_params['since'] = f"{since_date}T00:00:00Z"
                if until_date:
                    branch_params['until'] = f"{until_date}T23:59:59Z"
                
                # é¦–å…ˆå°è¯•ä¸æŒ‡å®šä½œè€…ï¼Œè·å–ä¸€äº›æäº¤çœ‹çœ‹å®é™…çš„ä½œè€…æ ¼å¼ï¼ˆä»…ç¬¬ä¸€ä¸ªåˆ†æ”¯ï¼‰
                if idx == 1:
                    try:
                        debug_params = {'ref_name': branch_obj.name, 'per_page': 20}
                        if since_date:
                            debug_params['since'] = f"{since_date}T00:00:00Z"
                        if until_date:
                            debug_params['until'] = f"{until_date}T23:59:59Z"
                        debug_commits = project.commits.list(**debug_params)
                        if debug_commits:
                            logger.info(f"è°ƒè¯•ï¼šåˆ†æ”¯ '{branch_obj.name}' çš„æäº¤ç¤ºä¾‹ï¼ˆä¸æŒ‡å®šä½œè€…ï¼Œæ—¥æœŸèŒƒå›´ {since_date or 'å…¨éƒ¨'} è‡³ {until_date or 'å…¨éƒ¨'}ï¼Œå…± {len(debug_commits)} æ¡ï¼‰ï¼š")
                            for dc_idx, dc in enumerate(debug_commits[:10], 1):
                                dc_author = getattr(dc, 'author_name', 'N/A')
                                dc_email = getattr(dc, 'author_email', 'N/A')
                                dc_date = getattr(dc, 'committed_date', 'N/A')
                                # æ ¼å¼åŒ–æ—¥æœŸ
                                if isinstance(dc_date, str):
                                    try:
                                        from datetime import datetime
                                        dc_date_obj = datetime.fromisoformat(dc_date.replace('Z', '+00:00'))
                                        dc_date_str = dc_date_obj.strftime('%Y-%m-%d %H:%M:%S')
                                        dc_date_local = dc_date_obj.strftime('%Y-%m-%d')
                                    except:
                                        dc_date_str = str(dc_date)
                                        dc_date_local = 'N/A'
                                else:
                                    dc_date_str = str(dc_date)
                                    dc_date_local = 'N/A'
                                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ä½œè€…
                                is_target = False
                                if author_name.lower() in str(dc_author).lower() or author_name.lower() in str(dc_email).lower():
                                    is_target = True
                                marker = " â† åŒ¹é…" if is_target else ""
                                logger.info(f"  æäº¤ {dc_idx}: ä½œè€…='{dc_author}' é‚®ç®±='{dc_email}' æ—¥æœŸ={dc_date_str} (UTCæ—¥æœŸ={dc_date_local}){marker}")
                        else:
                            logger.warning(f"è°ƒè¯•ï¼šåˆ†æ”¯ '{branch_obj.name}' åœ¨æŒ‡å®šæ—¥æœŸèŒƒå›´å†…ï¼ˆ{since_date or 'å…¨éƒ¨'} è‡³ {until_date or 'å…¨éƒ¨'}ï¼‰æ²¡æœ‰ä»»ä½•æäº¤ï¼ˆä¸æŒ‡å®šä½œè€…ï¼‰")
                            # å¦‚æœæŒ‡å®šäº†æ—¥æœŸèŒƒå›´ä½†æ²¡æœ‰æ‰¾åˆ°æäº¤ï¼Œå†æŸ¥è¯¢ä¸€æ¬¡ä¸é™åˆ¶æ—¥æœŸçš„ï¼Œçœ‹çœ‹æœ€è¿‘æœ‰å“ªäº›æäº¤
                            if since_date or until_date:
                                logger.info(f"è°ƒè¯•ï¼šæŸ¥è¯¢åˆ†æ”¯ '{branch_obj.name}' æœ€è¿‘çš„æäº¤ï¼ˆä¸é™åˆ¶æ—¥æœŸèŒƒå›´ï¼‰ï¼š")
                                try:
                                    debug_params_no_date = {'ref_name': branch_obj.name, 'per_page': 10}
                                    debug_commits_no_date = project.commits.list(**debug_params_no_date)
                                    if debug_commits_no_date:
                                        logger.info(f"  æ‰¾åˆ° {len(debug_commits_no_date)} æ¡æœ€è¿‘çš„æäº¤ï¼š")
                                        for dc_idx, dc in enumerate(debug_commits_no_date[:5], 1):
                                            dc_author = getattr(dc, 'author_name', 'N/A')
                                            dc_email = getattr(dc, 'author_email', 'N/A')
                                            dc_date = getattr(dc, 'committed_date', 'N/A')
                                            # æ ¼å¼åŒ–æ—¥æœŸ
                                            if isinstance(dc_date, str):
                                                try:
                                                    from datetime import datetime
                                                    dc_date_obj = datetime.fromisoformat(dc_date.replace('Z', '+00:00'))
                                                    dc_date_str = dc_date_obj.strftime('%Y-%m-%d %H:%M:%S')
                                                    dc_date_local = dc_date_obj.strftime('%Y-%m-%d')
                                                except:
                                                    dc_date_str = str(dc_date)
                                                    dc_date_local = 'N/A'
                                            else:
                                                dc_date_str = str(dc_date)
                                                dc_date_local = 'N/A'
                                            # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®æ ‡ä½œè€…
                                            is_target = False
                                            if author_name.lower() in str(dc_author).lower() or author_name.lower() in str(dc_email).lower():
                                                is_target = True
                                            marker = " â† åŒ¹é…" if is_target else ""
                                            logger.info(f"    æäº¤ {dc_idx}: ä½œè€…='{dc_author}' é‚®ç®±='{dc_email}' æ—¥æœŸ={dc_date_str} (UTCæ—¥æœŸ={dc_date_local}){marker}")
                                        logger.info(f"  æç¤ºï¼šå¦‚æœçœ‹åˆ°åŒ¹é…çš„æäº¤ï¼Œè¯·æ£€æŸ¥å…¶ UTC æ—¥æœŸæ˜¯å¦åœ¨æŸ¥è¯¢èŒƒå›´å†…")
                                    else:
                                        logger.warning(f"  è¯¥åˆ†æ”¯æ²¡æœ‰ä»»ä½•æäº¤è®°å½•")
                                except Exception as e:
                                    logger.debug(f"æŸ¥è¯¢æœ€è¿‘æäº¤å¤±è´¥: {e}")
                            else:
                                logger.warning(f"æç¤ºï¼šå¦‚æœç¡®å®šæœ‰æäº¤ï¼Œå¯èƒ½æ˜¯æ—¶åŒºé—®é¢˜ã€‚GitLab ä½¿ç”¨ UTC æ—¶é—´ï¼Œè¯·æ£€æŸ¥æäº¤çš„å®é™… UTC æ—¥æœŸ")
                    except Exception as e:
                        logger.warning(f"è°ƒè¯•æŸ¥è¯¢å¤±è´¥: {e}")
                
                branch_commits = []
                branch_page = 1
                while True:
                    branch_params['page'] = branch_page
                    page_commits = project.commits.list(**branch_params)
                    
                    if not page_commits:
                        # å¦‚æœç¬¬ä¸€é¡µç¬¬ä¸€ä¸ªåˆ†æ”¯æ²¡æœ‰ç»“æœï¼Œå°è¯•ä¸åŒçš„ author æ ¼å¼
                        if idx == 1 and branch_page == 1:
                            import re
                            email_match = re.search(r'<([^>]+)>', author_name)
                            if email_match:
                                email_only = email_match.group(1)
                                logger.info(f"å°è¯•ä½¿ç”¨é‚®ç®±æ ¼å¼æŸ¥è¯¢åˆ†æ”¯ '{branch_obj.name}': {email_only}")
                                branch_params_alt = branch_params.copy()
                                branch_params_alt['author'] = email_only
                                page_commits_alt = project.commits.list(**branch_params_alt)
                                if page_commits_alt:
                                    logger.info(f"ä½¿ç”¨é‚®ç®±æ ¼å¼æ‰¾åˆ° {len(page_commits_alt)} æ¡æäº¤")
                                    page_commits = page_commits_alt
                                    branch_params = branch_params_alt
                            # å°è¯•åªä½¿ç”¨åç§°éƒ¨åˆ†
                            name_match = re.match(r'^([^<]+)', author_name)
                            if name_match and not email_match:
                                name_only = name_match.group(1).strip()
                                logger.info(f"å°è¯•ä½¿ç”¨åç§°æ ¼å¼æŸ¥è¯¢åˆ†æ”¯ '{branch_obj.name}': {name_only}")
                                branch_params_alt = branch_params.copy()
                                branch_params_alt['author'] = name_only
                                page_commits_alt = project.commits.list(**branch_params_alt)
                                if page_commits_alt:
                                    logger.info(f"ä½¿ç”¨åç§°æ ¼å¼æ‰¾åˆ° {len(page_commits_alt)} æ¡æäº¤")
                                    page_commits = page_commits_alt
                                    branch_params = branch_params_alt
                        break
                    
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºç¬¬ä¸€æ¡æäº¤çš„ä½œè€…ä¿¡æ¯ï¼ˆä»…ç¬¬ä¸€é¡µç¬¬ä¸€ä¸ªåˆ†æ”¯ï¼‰
                    if idx == 1 and branch_page == 1 and page_commits:
                        first_commit = page_commits[0]
                        author_info = getattr(first_commit, 'author_name', 'N/A')
                        author_email = getattr(first_commit, 'author_email', 'N/A')
                        logger.debug(f"ç¤ºä¾‹æäº¤ä½œè€…: {author_info} <{author_email}>")
                        # å¦‚æœä½œè€…ä¸åŒ¹é…ï¼Œç»™å‡ºæç¤º
                        if author_name.lower() not in str(author_info).lower() and author_name.lower() not in str(author_email).lower():
                            logger.warning(f"æ³¨æ„: æŸ¥è¯¢çš„ä½œè€… '{author_name}' ä¸è¿”å›çš„æäº¤ä½œè€… '{author_info} <{author_email}>' ä¸åŒ¹é…")
                            logger.warning(f"å»ºè®®: å°è¯•ä½¿ç”¨ '{author_info}' æˆ– '{author_email}' ä½œä¸ºæäº¤è€…åç§°")
                    
                    branch_commits.extend(page_commits)
                    
                    if len(page_commits) < per_page:
                        break
                    
                    branch_page += 1
                
                if branch_commits:
                    logger.info(f"[{idx}/{len(branches)}] åˆ†æ”¯ '{branch_obj.name}': æ‰¾åˆ° {len(branch_commits)} æ¡æäº¤")
                    all_commits.extend(branch_commits)
                else:
                    # è°ƒè¯•ï¼šå¦‚æœæ²¡æ‰¾åˆ°æäº¤ï¼Œè®°å½•ä¸€ä¸‹ï¼ˆä»…åœ¨è°ƒè¯•æ¨¡å¼ä¸‹ï¼‰
                    logger.debug(f"[{idx}/{len(branches)}] åˆ†æ”¯ '{branch_obj.name}': æœªæ‰¾åˆ°æäº¤")
            except Exception as e:
                # å¿½ç•¥æƒé™ä¸è¶³ç­‰é”™è¯¯
                logger.debug(f"æŸ¥è¯¢åˆ†æ”¯ '{branch_obj.name}' æ—¶å‡ºé”™: {str(e)}")
                continue
        
        # å»é‡ï¼ˆåŒä¸€ä¸ªæäº¤å¯èƒ½åœ¨å¤šä¸ªåˆ†æ”¯ä¸Šï¼‰
        seen_ids = set()
        unique_commits = []
        for commit in all_commits:
            if commit.id not in seen_ids:
                seen_ids.add(commit.id)
                unique_commits.append(commit)
        
        logger.info(f"å…±è·å–åˆ° {len(unique_commits)} æ¡æäº¤è®°å½•ï¼ˆéå†äº† {len(branches)} ä¸ªåˆ†æ”¯ï¼‰")
        return unique_commits
    
    # æ·»åŠ æ—¥æœŸèŒƒå›´
    if since_date:
        params['since'] = f"{since_date}T00:00:00Z"
    if until_date:
        params['until'] = f"{until_date}T23:59:59Z"
    
    try:
        while True:
            params['page'] = page
            page_commits = project.commits.list(**params)
            
            if not page_commits:
                break
            
            commits.extend(page_commits)
            logger.info(f"å·²è·å– {len(commits)} æ¡æäº¤è®°å½•...")
            
            # å¦‚æœè¿”å›çš„æäº¤æ•°å°‘äºæ¯é¡µæ•°é‡ï¼Œè¯´æ˜å·²ç»æ˜¯æœ€åä¸€é¡µ
            if len(page_commits) < per_page:
                break
            
            page += 1
        
        logger.info(f"å…±è·å–åˆ° {len(commits)} æ¡æäº¤è®°å½•")
        return commits
    
    except Exception as e:
        logger.error(f"è·å–æäº¤è®°å½•å¤±è´¥: {str(e)}")
        raise


def group_commits_by_date(commits):
    """
    æŒ‰æ—¥æœŸåˆ†ç»„æäº¤
    
    Args:
        commits: æäº¤åˆ—è¡¨
    
    Returns:
        dict: æŒ‰æ—¥æœŸåˆ†ç»„çš„æäº¤å­—å…¸ï¼Œæ ¼å¼ï¼š{date: [commits]}
    """
    grouped = defaultdict(list)
    
    for commit in commits:
        # è§£ææäº¤æ—¥æœŸ
        commit_date = commit.committed_date
        if isinstance(commit_date, str):
            # è§£æ ISO 8601 æ ¼å¼æ—¥æœŸ
            date_obj = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
        else:
            date_obj = commit_date
        
        # æå–æ—¥æœŸéƒ¨åˆ†ï¼ˆYYYY-MM-DDï¼‰
        date_str = date_obj.strftime('%Y-%m-%d')
        grouped[date_str].append(commit)
    
    # æŒ‰æ—¥æœŸæ’åº
    sorted_dates = sorted(grouped.keys(), reverse=True)
    return {date: grouped[date] for date in sorted_dates}


def get_all_projects(gl, owned=False, membership=False):
    """
    è·å–ç”¨æˆ·æœ‰æƒé™è®¿é—®çš„æ‰€æœ‰é¡¹ç›®
    
    Args:
        gl: GitLab å®¢æˆ·ç«¯å®ä¾‹
        owned: æ˜¯å¦åªè·å–ç”¨æˆ·æ‹¥æœ‰çš„é¡¹ç›®ï¼ˆé»˜è®¤ï¼šFalseï¼‰
        membership: æ˜¯å¦åªè·å–ç”¨æˆ·æ˜¯æˆå‘˜çš„é¡¹ç›®ï¼ˆé»˜è®¤ï¼šFalseï¼‰
    
    Returns:
        list: é¡¹ç›®åˆ—è¡¨
    """
    logger.info("å¼€å§‹è·å–æ‰€æœ‰é¡¹ç›®åˆ—è¡¨...")
    projects = []
    
    try:
        # è·å–é¡¹ç›®åˆ—è¡¨
        params = {'per_page': 100}
        if owned:
            params['owned'] = True
        if membership:
            params['membership'] = True
        
        page = 1
        while True:
            params['page'] = page
            page_projects = gl.projects.list(**params)
            
            if not page_projects:
                break
            
            projects.extend(page_projects)
            logger.info(f"å·²è·å– {len(projects)} ä¸ªé¡¹ç›®...")
            
            if len(page_projects) < 100:
                break
            
            page += 1
        
        logger.info(f"å…±è·å–åˆ° {len(projects)} ä¸ªé¡¹ç›®")
        return projects
    
    except Exception as e:
        logger.error(f"è·å–é¡¹ç›®åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise


def scan_all_projects(gl, author_name, since_date=None, until_date=None, branch=None):
    """
    æ‰«ææ‰€æœ‰é¡¹ç›®ï¼ŒæŸ¥æ‰¾æŒ‡å®šæäº¤è€…çš„æäº¤
    
    Args:
        gl: GitLab å®¢æˆ·ç«¯å®ä¾‹
        author_name: æäº¤è€…å§“åæˆ–é‚®ç®±
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        branch: åˆ†æ”¯åç§°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        dict: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸ï¼Œæ ¼å¼ï¼š{project_path: {'project': project, 'commits': commits}}
    """
    logger.info(f"å¼€å§‹æ‰«ææ‰€æœ‰é¡¹ç›®ï¼ŒæŸ¥æ‰¾æäº¤è€… '{author_name}' çš„æäº¤...")
    
    # è·å–æ‰€æœ‰é¡¹ç›®
    projects = get_all_projects(gl)
    
    results = {}
    total_commits = 0
    
    for idx, project in enumerate(projects, 1):
        project_path = project.path_with_namespace
        logger.info(f"[{idx}/{len(projects)}] æ­£åœ¨æ‰«æé¡¹ç›®: {project_path}")
        
        try:
            # è·å–è¯¥é¡¹ç›®çš„æäº¤
            commits = get_commits_by_author(
                project,
                author_name,
                since_date=since_date,
                until_date=until_date,
                branch=branch
            )
            
            if commits:
                results[project_path] = {
                    'project': project,
                    'commits': commits
                }
                total_commits += len(commits)
                logger.info(f"  âœ“ æ‰¾åˆ° {len(commits)} æ¡æäº¤")
            # ä¸è¾“å‡ºæœªæ‰¾åˆ°æäº¤çš„ä¿¡æ¯ï¼Œå‡å°‘æ—¥å¿—å™ªéŸ³
        
        except Exception as e:
            # åªè®°å½•é‡è¦é”™è¯¯ï¼Œå¿½ç•¥æƒé™ä¸è¶³ç­‰å¸¸è§é”™è¯¯
            error_msg = str(e)
            if '403' in error_msg or '401' in error_msg or 'Not Found' in error_msg:
                logger.debug(f"  è·³è¿‡é¡¹ç›® {project_path}ï¼ˆæ— æƒé™æˆ–ä¸å­˜åœ¨ï¼‰")
            else:
                logger.warning(f"  æ‰«æé¡¹ç›® {project_path} æ—¶å‡ºé”™: {error_msg}")
            continue
    
    logger.info(f"æ‰«æå®Œæˆï¼Œå…±åœ¨ {len(results)} ä¸ªé¡¹ç›®ä¸­æ‰¾åˆ° {total_commits} æ¡æäº¤")
    return results


def generate_markdown_log(grouped_commits, author_name, repo_name=None, project=None):
    """
    ç”Ÿæˆ Markdown æ ¼å¼çš„æ—¥å¿—
    
    Args:
        grouped_commits: æŒ‰æ—¥æœŸåˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        repo_name: ä»“åº“åç§°ï¼ˆå¯é€‰ï¼‰
        project: GitLab é¡¹ç›®å¯¹è±¡ï¼ˆå¯é€‰ï¼Œç”¨äºè·å–è¯¦ç»†commitä¿¡æ¯ï¼‰
    
    Returns:
        str: Markdown æ ¼å¼çš„æ—¥å¿—å†…å®¹
    """
    lines = []
    
    # æ ‡é¢˜
    if repo_name:
        lines.append(f"# {repo_name} - {author_name} æäº¤æ—¥å¿—\n")
    else:
        lines.append(f"# {author_name} æäº¤æ—¥å¿—\n")
    
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    lines.append(f"**æ€»æäº¤æ•°**: {sum(len(commits) for commits in grouped_commits.values())}\n")
    lines.append(f"**æäº¤å¤©æ•°**: {len(grouped_commits)}\n")
    lines.append("\n---\n\n")
    
    # æŒ‰æ—¥æœŸè¾“å‡ºæäº¤
    for date, commits in grouped_commits.items():
        # æ—¥æœŸæ ‡é¢˜
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        date_formatted = date_obj.strftime('%Yå¹´%mæœˆ%dæ—¥')
        lines.append(f"## {date_formatted} ({date})\n")
        
        # å½“æ—¥ç»Ÿè®¡
        lines.append(f"**æäº¤æ•°**: {len(commits)}\n\n")
        
        # æäº¤åˆ—è¡¨
        for idx, commit in enumerate(commits, 1):
            # è·å–è¯¦ç»†commitä¿¡æ¯
            if project:
                try:
                    details = get_commit_details(project, commit)
                    short_message = details['short_message']
                    full_message = details['full_message']
                    stats = details['stats']
                    changed_files = details['changed_files']
                except Exception as e:
                    logger.debug(f"è·å–commitè¯¦æƒ…å¤±è´¥: {str(e)}")
                    short_message = commit.message.split('\n')[0] if commit.message else ''
                    full_message = commit.message or ''
                    stats = None
                    changed_files = []
            else:
                short_message = commit.message.split('\n')[0] if commit.message else ''
                full_message = commit.message or ''
                stats = None
                changed_files = []
            
            commit_id = commit.id[:8]  # çŸ­æäº¤ ID
            commit_url = getattr(commit, 'web_url', '')
            
            lines.append(f"### {idx}. [{commit_id}]({commit_url}) {short_message}\n")
            
            # æäº¤æ—¶é—´
            commit_time = commit.committed_date
            if isinstance(commit_time, str):
                time_obj = datetime.fromisoformat(commit_time.replace('Z', '+00:00'))
            else:
                time_obj = commit_time
            time_str = time_obj.strftime('%H:%M:%S')
            lines.append(f"**æ—¶é—´**: {time_str}\n")
            
            # æ˜¾ç¤ºå®Œæ•´çš„commit messageï¼ˆå¦‚æœæœ‰å¤šè¡Œï¼‰
            if full_message and '\n' in full_message:
                lines.append(f"**å®Œæ•´æäº¤ä¿¡æ¯**:\n```\n{full_message}\n```\n")
            
            # æ˜¾ç¤ºä»£ç è¡Œæ•°ç»Ÿè®¡
            if stats:
                lines.append(f"**ä»£ç å˜æ›´**: +{stats.get('additions', 0)} -{stats.get('deletions', 0)} (æ€»è®¡: {stats.get('total', 0)} è¡Œ)\n")
            elif hasattr(commit, 'stats') and commit.stats:
                try:
                    commit_stats = commit.stats
                    if isinstance(commit_stats, dict):
                        lines.append(f"**ä»£ç å˜æ›´**: +{commit_stats.get('additions', 0)} -{commit_stats.get('deletions', 0)}\n")
                except:
                    pass
            
            # æ˜¾ç¤ºæ–‡ä»¶å˜æ›´åˆ—è¡¨
            if changed_files:
                lines.append(f"**å˜æ›´æ–‡ä»¶** ({len(changed_files)} ä¸ª):\n")
                for file_info in changed_files[:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ªæ–‡ä»¶
                    file_path = file_info.get('new_path') or file_info.get('old_path') or file_info.get('path', '')
                    if file_path:
                        lines.append(f"- `{file_path}`\n")
                if len(changed_files) > 10:
                    lines.append(f"- ... è¿˜æœ‰ {len(changed_files) - 10} ä¸ªæ–‡ä»¶\n")
            
            lines.append("\n")
        
        lines.append("---\n\n")
    
    return ''.join(lines)


def generate_multi_project_markdown(all_results, author_name, since_date=None, until_date=None):
    """
    ç”Ÿæˆå¤šé¡¹ç›®æ±‡æ€»çš„ Markdown æ ¼å¼æ—¥å¿—
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: Markdown æ ¼å¼çš„æ—¥å¿—å†…å®¹
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {author_name} - æ‰€æœ‰é¡¹ç›®æäº¤æ±‡æ€»æ—¥å¿—\n")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    
    # æ—¥æœŸèŒƒå›´
    if since_date and until_date:
        lines.append(f"**æ—¥æœŸèŒƒå›´**: {since_date} è‡³ {until_date}\n")
    elif since_date:
        lines.append(f"**èµ·å§‹æ—¥æœŸ**: {since_date}\n")
    elif until_date:
        lines.append(f"**ç»“æŸæ—¥æœŸ**: {until_date}\n")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_projects = len(all_results)
    total_commits = sum(len(result['commits']) for result in all_results.values())
    
    # æŒ‰æ—¥æœŸæ±‡æ€»æ‰€æœ‰æäº¤
    all_commits_by_date = defaultdict(list)
    for project_path, result in all_results.items():
        commits = result['commits']
        for commit in commits:
            commit_date = commit.committed_date
            if isinstance(commit_date, str):
                date_obj = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
            else:
                date_obj = commit_date
            date_str = date_obj.strftime('%Y-%m-%d')
            all_commits_by_date[date_str].append({
                'project': project_path,
                'commit': commit
            })
    
    lines.append(f"**æ¶‰åŠé¡¹ç›®æ•°**: {total_projects}\n")
    lines.append(f"**æ€»æäº¤æ•°**: {total_commits}\n")
    lines.append(f"**æäº¤å¤©æ•°**: {len(all_commits_by_date)}\n")
    lines.append("\n---\n\n")
    
    # æŒ‰æ—¥æœŸè¾“å‡ºæäº¤
    sorted_dates = sorted(all_commits_by_date.keys(), reverse=True)
    for date in sorted_dates:
        date_obj = datetime.strptime(date, '%Y-%m-%d')
        date_formatted = date_obj.strftime('%Yå¹´%mæœˆ%dæ—¥')
        lines.append(f"## {date_formatted} ({date})\n")
        
        commits_on_date = all_commits_by_date[date]
        lines.append(f"**æäº¤æ•°**: {len(commits_on_date)}\n\n")
        
        # æŒ‰é¡¹ç›®åˆ†ç»„
        commits_by_project = defaultdict(list)
        for item in commits_on_date:
            commits_by_project[item['project']].append(item['commit'])
        
        # è¾“å‡ºæ¯ä¸ªé¡¹ç›®çš„æäº¤
        for project_path in sorted(commits_by_project.keys()):
            project_commits = commits_by_project[project_path]
            project_info = all_results[project_path]['project']
            
            lines.append(f"### ğŸ“¦ {project_path}\n")
            lines.append(f"**é¡¹ç›®**: [{project_info.name}]({project_info.web_url})\n")
            lines.append(f"**æäº¤æ•°**: {len(project_commits)}\n\n")
            
            # æŒ‰æ—¶é—´æ’åº
            project_commits.sort(key=lambda c: c.committed_date, reverse=True)
            
            # è·å–é¡¹ç›®å¯¹è±¡ç”¨äºè·å–è¯¦ç»†commitä¿¡æ¯
            project = all_results[project_path]['project']
            
            for idx, commit in enumerate(project_commits, 1):
                # è·å–è¯¦ç»†commitä¿¡æ¯
                try:
                    details = get_commit_details(project, commit)
                    short_message = details['short_message']
                    full_message = details['full_message']
                    stats = details['stats']
                    changed_files = details['changed_files']
                except Exception as e:
                    logger.debug(f"è·å–commitè¯¦æƒ…å¤±è´¥: {str(e)}")
                    short_message = commit.message.split('\n')[0] if commit.message else ''
                    full_message = commit.message or ''
                    stats = None
                    changed_files = []
                
                commit_id = commit.id[:8]
                commit_url = getattr(commit, 'web_url', '')
                
                lines.append(f"#### {idx}. [{commit_id}]({commit_url}) {short_message}\n")
                
                commit_time = commit.committed_date
                if isinstance(commit_time, str):
                    time_obj = datetime.fromisoformat(commit_time.replace('Z', '+00:00'))
                else:
                    time_obj = commit_time
                time_str = time_obj.strftime('%H:%M:%S')
                lines.append(f"**æ—¶é—´**: {time_str}\n")
                
                # æ˜¾ç¤ºå®Œæ•´çš„commit messageï¼ˆå¦‚æœæœ‰å¤šè¡Œï¼‰
                if full_message and '\n' in full_message:
                    lines.append(f"**å®Œæ•´æäº¤ä¿¡æ¯**:\n```\n{full_message}\n```\n")
                
                # æ˜¾ç¤ºä»£ç è¡Œæ•°ç»Ÿè®¡
                if stats:
                    lines.append(f"**ä»£ç å˜æ›´**: +{stats.get('additions', 0)} -{stats.get('deletions', 0)} (æ€»è®¡: {stats.get('total', 0)} è¡Œ)\n")
                elif hasattr(commit, 'stats') and commit.stats:
                    try:
                        commit_stats = commit.stats
                        if isinstance(commit_stats, dict):
                            lines.append(f"**ä»£ç å˜æ›´**: +{commit_stats.get('additions', 0)} -{commit_stats.get('deletions', 0)}\n")
                    except:
                        pass
                
                # æ˜¾ç¤ºæ–‡ä»¶å˜æ›´åˆ—è¡¨ï¼ˆæœ€å¤šæ˜¾ç¤º5ä¸ªï¼‰
                if changed_files:
                    lines.append(f"**å˜æ›´æ–‡ä»¶** ({len(changed_files)} ä¸ª):\n")
                    for file_info in changed_files[:5]:
                        file_path = file_info.get('new_path') or file_info.get('old_path') or file_info.get('path', '')
                        if file_path:
                            lines.append(f"- `{file_path}`\n")
                    if len(changed_files) > 5:
                        lines.append(f"- ... è¿˜æœ‰ {len(changed_files) - 5} ä¸ªæ–‡ä»¶\n")
                
                lines.append("\n")
            
            lines.append("---\n\n")
    
    return ''.join(lines)


def analyze_commit_type(commit_message):
    """
    åˆ†ææäº¤ç±»å‹
    
    Args:
        commit_message: æäº¤ä¿¡æ¯
    
    Returns:
        tuple: (ç±»å‹, emoji)
    """
    message_lower = commit_message.lower()
    
    # ä¼˜å…ˆæ£€æŸ¥å‰ç¼€ï¼ˆæ›´å‡†ç¡®ï¼‰
    if message_lower.startswith('fix') or message_lower.startswith('ä¿®å¤'):
        return ('Bugä¿®å¤', 'ğŸ›')
    elif message_lower.startswith('feat') or message_lower.startswith('æ–°å¢') or message_lower.startswith('æ·»åŠ '):
        return ('åŠŸèƒ½å¼€å‘', 'âœ¨')
    elif message_lower.startswith('refactor') or message_lower.startswith('é‡æ„'):
        return ('ä»£ç é‡æ„', 'â™»ï¸')
    elif message_lower.startswith('chore') or message_lower.startswith('åˆ é™¤') or message_lower.startswith('æ¸…ç†'):
        return ('ä»£ç ç»´æŠ¤', 'ğŸ”§')
    elif message_lower.startswith('docs') or message_lower.startswith('æ–‡æ¡£'):
        return ('æ–‡æ¡£æ›´æ–°', 'ğŸ“')
    elif message_lower.startswith('style') or message_lower.startswith('æ ·å¼'):
        return ('æ ·å¼è°ƒæ•´', 'ğŸ’„')
    elif message_lower.startswith('test') or message_lower.startswith('æµ‹è¯•'):
        return ('æµ‹è¯•ç›¸å…³', 'âœ…')
    # ç„¶åæ£€æŸ¥å…³é”®è¯
    elif 'ä¿®å¤' in commit_message or 'è§£å†³' in commit_message or 'bug' in message_lower:
        return ('Bugä¿®å¤', 'ğŸ›')
    elif 'æ–°å¢' in commit_message or 'æ·»åŠ ' in commit_message:
        return ('åŠŸèƒ½å¼€å‘', 'âœ¨')
    elif 'é‡æ„' in commit_message or ('ä¼˜åŒ–' in commit_message and 'ä¿®å¤' not in commit_message):
        return ('ä»£ç é‡æ„', 'â™»ï¸')
    else:
        return ('å…¶ä»–', 'ğŸ“Œ')


def get_commit_details(project, commit, timeout=10, max_files=50, max_message_length=5000):
    """
    è·å–å•ä¸ªæäº¤çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå¸¦è¶…æ—¶å’Œå¼‚å¸¸å¤„ç†ï¼‰
    
    Args:
        project: GitLab é¡¹ç›®å¯¹è±¡
        commit: GitLab commit å¯¹è±¡
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
        max_files: æœ€å¤§æ–‡ä»¶æ•°é‡ï¼Œé»˜è®¤50ä¸ª
        max_message_length: æœ€å¤§æ¶ˆæ¯é•¿åº¦ï¼Œé»˜è®¤5000å­—ç¬¦
    
    Returns:
        dict: åŒ…å«å®Œæ•´ä¿¡æ¯çš„å­—å…¸
            - full_message: å®Œæ•´çš„commit messageï¼ˆå¤šè¡Œï¼Œå·²æˆªæ–­ï¼‰
            - short_message: ç¬¬ä¸€è¡Œcommit message
            - changed_files: æ–‡ä»¶å˜æ›´åˆ—è¡¨ï¼ˆå·²é™åˆ¶æ•°é‡ï¼‰
            - stats: ä»£ç è¡Œæ•°ç»Ÿè®¡
            - author: ä½œè€…ä¿¡æ¯
            - committed_date: æäº¤æ—¶é—´
    """
    import signal
    
    # é™åˆ¶commit messageé•¿åº¦
    full_message = commit.message or ''
    if len(full_message) > max_message_length:
        full_message = full_message[:max_message_length] + '\n... (æ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­)'
        logger.debug(f"Commit {commit.id[:8]} æ¶ˆæ¯è¿‡é•¿ï¼Œå·²æˆªæ–­è‡³ {max_message_length} å­—ç¬¦")
    
    details = {
        'full_message': full_message,
        'short_message': full_message.split('\n')[0] if full_message else '',
        'changed_files': [],
        'stats': None,
        'author': getattr(commit, 'author_name', ''),
        'committed_date': commit.committed_date,
        'web_url': getattr(commit, 'web_url', '')
    }
    
    # è¶…æ—¶å¤„ç†å‡½æ•°
    def timeout_handler(signum, frame):
        raise TimeoutError(f"è·å–commitè¯¦æƒ…è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰")
    
    try:
        # è®¾ç½®è¶…æ—¶ï¼ˆä»…Unixç³»ç»Ÿï¼‰
        if hasattr(signal, 'SIGALRM'):
            signal.signal(signal.SIGALRM, timeout_handler)
            signal.alarm(timeout)
        
        try:
            # å°è¯•è·å–è¯¦ç»†çš„commitä¿¡æ¯
            detailed_commit = project.commits.get(commit.id)
            
            # è·å–æ–‡ä»¶å˜æ›´åˆ—è¡¨ï¼ˆé™åˆ¶æ•°é‡å’Œå¤§å°ï¼‰
            try:
                if hasattr(detailed_commit, 'diff'):
                    diffs = detailed_commit.diff()
                    file_count = 0
                    for diff in diffs:
                        if file_count >= max_files:
                            logger.debug(f"Commit {commit.id[:8]} æ–‡ä»¶æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œä»…æ˜¾ç¤ºå‰ {max_files} ä¸ª")
                            break
                        
                        try:
                            diff_text = getattr(diff, 'diff', '')
                            # é™åˆ¶å•ä¸ªdiffçš„å¤§å°
                            if diff_text and len(diff_text) > 10000:
                                diff_text = diff_text[:10000] + '\n... (diffè¿‡é•¿ï¼Œå·²æˆªæ–­)'
                            
                            file_info = {
                                'path': getattr(diff, 'new_path', getattr(diff, 'old_path', '')),
                                'old_path': getattr(diff, 'old_path', ''),
                                'new_path': getattr(diff, 'new_path', ''),
                                'diff': diff_text[:500] if diff_text else ''  # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
                            }
                            details['changed_files'].append(file_info)
                            file_count += 1
                        except Exception as e:
                            logger.debug(f"å¤„ç†å•ä¸ªæ–‡ä»¶diffå¤±è´¥: {str(e)}")
                            continue
                    
                    if len(diffs) > max_files:
                        details['changed_files'].append({
                            'path': f'... è¿˜æœ‰ {len(diffs) - max_files} ä¸ªæ–‡ä»¶æœªæ˜¾ç¤º',
                            'old_path': '',
                            'new_path': '',
                            'diff': ''
                        })
            except TimeoutError:
                logger.warning(f"è·å–commit {commit.id[:8]} æ–‡ä»¶å˜æ›´åˆ—è¡¨è¶…æ—¶")
            except Exception as e:
                logger.debug(f"è·å–æ–‡ä»¶å˜æ›´åˆ—è¡¨å¤±è´¥: {str(e)}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            try:
                if hasattr(detailed_commit, 'stats') and detailed_commit.stats:
                    stats = detailed_commit.stats
                    if isinstance(stats, dict):
                        details['stats'] = {
                            'additions': stats.get('additions', 0),
                            'deletions': stats.get('deletions', 0),
                            'total': stats.get('total', 0)
                        }
            except Exception as e:
                logger.debug(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        finally:
            # å–æ¶ˆè¶…æ—¶
            if hasattr(signal, 'SIGALRM'):
                signal.alarm(0)
    
    except TimeoutError as e:
        logger.warning(f"è·å–commit {commit.id[:8]} è¯¦æƒ…è¶…æ—¶: {str(e)}")
    except Exception as e:
        logger.debug(f"è·å–è¯¦ç»†commitä¿¡æ¯å¤±è´¥: {str(e)}")
        # é™çº§ï¼šä½¿ç”¨åŸºæœ¬ä¿¡æ¯
        try:
            if hasattr(commit, 'stats') and commit.stats:
                stats = commit.stats
                if isinstance(stats, dict):
                    details['stats'] = {
                        'additions': stats.get('additions', 0),
                        'deletions': stats.get('deletions', 0),
                        'total': stats.get('total', 0)
                    }
        except:
            pass
    
    return details


def get_commit_stats(project, commit):
    """
    è·å–å•ä¸ªæäº¤çš„ä»£ç è¡Œæ•°ç»Ÿè®¡
    
    Args:
        project: GitLab é¡¹ç›®å¯¹è±¡
        commit: GitLab commit å¯¹è±¡
    
    Returns:
        dict: åŒ…å« additions, deletions, total çš„å­—å…¸ï¼Œå¦‚æœæ— æ³•è·å–åˆ™è¿”å› None
    """
    try:
        # æ–¹æ³•1: å°è¯•ç›´æ¥è®¿é—® stats å±æ€§
        if hasattr(commit, 'stats') and commit.stats:
            stats = commit.stats
            if isinstance(stats, dict):
                return {
                    'additions': stats.get('additions', 0),
                    'deletions': stats.get('deletions', 0),
                    'total': stats.get('total', 0)
                }
        
        # æ–¹æ³•2: å°è¯•é€šè¿‡ API è·å–è¯¦ç»† commit ä¿¡æ¯
        try:
            detailed_commit = project.commits.get(commit.id)
            if hasattr(detailed_commit, 'stats') and detailed_commit.stats:
                stats = detailed_commit.stats
                if isinstance(stats, dict):
                    return {
                        'additions': stats.get('additions', 0),
                        'deletions': stats.get('deletions', 0),
                        'total': stats.get('total', 0)
                    }
        except Exception:
            pass
        
        # æ–¹æ³•3: å°è¯•é€šè¿‡ diff è®¡ç®—ï¼ˆæ€§èƒ½è¾ƒä½ï¼Œä½œä¸ºæœ€åæ‰‹æ®µï¼‰
        try:
            diffs = commit.diff()
            additions = 0
            deletions = 0
            for diff in diffs:
                if hasattr(diff, 'diff'):
                    diff_text = diff.diff
                    if diff_text:
                        # è®¡ç®—æ–°å¢å’Œåˆ é™¤çš„è¡Œæ•°
                        for line in diff_text.split('\n'):
                            if line.startswith('+') and not line.startswith('+++'):
                                additions += 1
                            elif line.startswith('-') and not line.startswith('---'):
                                deletions += 1
            return {
                'additions': additions,
                'deletions': deletions,
                'total': additions + deletions
            }
        except Exception:
            pass
        
        return None
    except Exception as e:
        logger.debug(f"è·å–æäº¤ {commit.id[:8]} çš„ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        return None


def calculate_code_statistics(all_results, since_date=None, until_date=None):
    """
    è®¡ç®—æ€»ä½“ä»£ç è¡Œæ•°ç»Ÿè®¡
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        dict: åŒ…å«æ€»æ–°å¢è¡Œæ•°ã€æ€»åˆ é™¤è¡Œæ•°ã€å‡€å¢è¡Œæ•°ã€å¹³å‡æ¯æ¬¡æäº¤ä»£ç è¡Œæ•°ç­‰ç»Ÿè®¡ä¿¡æ¯
    """
    total_additions = 0
    total_deletions = 0
    total_commits_with_stats = 0
    total_commits = 0
    
    # ç”¨äºç¼“å­˜å·²è·å–çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé¿å…é‡å¤APIè°ƒç”¨
    stats_cache = {}
    
    for project_path, result in all_results.items():
        project = result['project']
        commits = result['commits']
        
        for commit in commits:
            total_commits += 1
            
            # å°è¯•ä»ç¼“å­˜è·å–
            commit_id = commit.id
            if commit_id in stats_cache:
                stats = stats_cache[commit_id]
            else:
                stats = get_commit_stats(project, commit)
                stats_cache[commit_id] = stats
            
            if stats:
                total_additions += stats.get('additions', 0)
                total_deletions += stats.get('deletions', 0)
                total_commits_with_stats += 1
    
    net_lines = total_additions - total_deletions
    avg_lines_per_commit = (total_additions + total_deletions) / total_commits_with_stats if total_commits_with_stats > 0 else 0
    
    return {
        'total_additions': total_additions,
        'total_deletions': total_deletions,
        'net_lines': net_lines,
        'total_commits': total_commits,
        'commits_with_stats': total_commits_with_stats,
        'avg_lines_per_commit': round(avg_lines_per_commit, 2),
        'stats_availability': total_commits_with_stats / total_commits if total_commits > 0 else 0
    }


def calculate_scores(all_results, since_date=None, until_date=None):
    """
    è®¡ç®—5ä¸ªç»´åº¦çš„è¯„åˆ†ï¼ˆæ»¡åˆ†100åˆ†ï¼‰
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼Œæ ¼å¼ï¼šYYYY-MM-DDï¼‰
    
    Returns:
        dict: åŒ…å«5ä¸ªç»´åº¦è¯„åˆ†çš„å­—å…¸
    """
    from collections import defaultdict
    import statistics
    from datetime import datetime, timedelta
    
    # æ”¶é›†æ‰€æœ‰æäº¤
    all_commits = []
    all_dates = set()
    projects_set = set()
    
    # ä¿®å¤ç±»å…³é”®è¯
    fix_keywords = ['fix', 'bug', 'ä¿®å¤', 'æŠ¥é”™', 'è§£å†³', 'error', 'issue', 'bugfix', 'hotfix']
    # åŠŸèƒ½ç±»å…³é”®è¯
    feat_keywords = ['feat', 'add', 'å¼€å‘', 'æ–°å¢', 'feature', 'implement', 'å®ç°', 'å¼€å‘', 'æ·»åŠ ']
    
    fix_commits = 0
    feat_commits = 0
    
    for project_path, result in all_results.items():
        projects_set.add(project_path)
        commits = result['commits']
        
        for commit in commits:
            all_commits.append(commit)
            
            # è§£ææ—¥æœŸ
            commit_date = commit.committed_date
            if isinstance(commit_date, str):
                date_obj = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
            else:
                date_obj = commit_date
            date_str = date_obj.strftime('%Y-%m-%d')
            all_dates.add(date_str)
            
            # æ£€æŸ¥æäº¤ç±»å‹
            commit_message = commit.message.lower()
            is_fix = any(keyword in commit_message for keyword in fix_keywords)
            is_feat = any(keyword in commit_message for keyword in feat_keywords)
            
            if is_fix:
                fix_commits += 1
            if is_feat:
                feat_commits += 1
    
    total_commits = len(all_commits)
    active_days = len(all_dates)
    
    # ç¡®å®šæ—¥æœŸèŒƒå›´
    if since_date and until_date:
        try:
            start_date = datetime.strptime(since_date, '%Y-%m-%d')
            end_date = datetime.strptime(until_date, '%Y-%m-%d')
        except ValueError:
            # å¦‚æœæ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨å®é™…æäº¤çš„æ—¥æœŸèŒƒå›´
            if all_dates:
                sorted_dates = sorted(all_dates)
                start_date = datetime.strptime(sorted_dates[0], '%Y-%m-%d')
                end_date = datetime.strptime(sorted_dates[-1], '%Y-%m-%d')
            else:
                start_date = datetime.now()
                end_date = datetime.now()
    elif all_dates:
        sorted_dates = sorted(all_dates)
        start_date = datetime.strptime(sorted_dates[0], '%Y-%m-%d')
        end_date = datetime.strptime(sorted_dates[-1], '%Y-%m-%d')
    else:
        start_date = datetime.now()
        end_date = datetime.now()
    
    total_days = (end_date - start_date).days + 1 if (end_date - start_date).days >= 0 else 1
    
    # 1. å‹¤å¥‹åº¦ (Diligence) - æ»¡åˆ†100
    # æ´»è·ƒå¤©æ•°å æ¯”ï¼šæ´»è·ƒå¤©æ•° / æ€»å¤©æ•° * 50åˆ†
    active_days_score = min(50, (active_days / total_days) * 50) if total_days > 0 else 0
    
    # æäº¤é¢‘ç‡ï¼š(æ€»æäº¤æ•° / æ€»å¤©æ•°) / åŸºå‡†é¢‘ç‡ * 50åˆ†ï¼ˆåŸºå‡†é¢‘ç‡ï¼š1æ¬¡/å¤©ï¼‰
    base_frequency = 1.0
    actual_frequency = total_commits / total_days if total_days > 0 else 0
    frequency_score = min(50, (actual_frequency / base_frequency) * 50)
    
    diligence_score = min(100, active_days_score + frequency_score)
    
    # 2. ç¨³å®šæ€§ (Stability) - æ»¡åˆ†100
    # è®¡ç®—æ¯æœˆæäº¤æ•°
    monthly_commits = defaultdict(int)
    for commit in all_commits:
        commit_date = commit.committed_date
        if isinstance(commit_date, str):
            date_obj = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
        else:
            date_obj = commit_date
        month_key = date_obj.strftime('%Y-%m')
        monthly_commits[month_key] += 1
    
    cv = 0
    mean_commits = 0
    
    if len(monthly_commits) > 0:
        commit_counts = list(monthly_commits.values())
        if len(commit_counts) > 1:
            mean_commits = statistics.mean(commit_counts)
            if mean_commits > 0:
                std_commits = statistics.stdev(commit_counts)
                cv = std_commits / mean_commits  # ç¦»æ•£ç³»æ•°
                base_cv = 1.0
                stability_score = 100 * (1 - min(1, cv / base_cv))
            else:
                stability_score = 0
        else:
            stability_score = 100  # åªæœ‰ä¸€ä¸ªæœˆï¼Œè®¤ä¸ºéå¸¸ç¨³å®š
    else:
        stability_score = 0
    
    # å¦‚æœæ¯æœˆéƒ½æœ‰æäº¤ï¼Œç»™äºˆé¢å¤–åŠ åˆ†ï¼ˆæœ€å¤š10åˆ†ï¼‰
    if since_date and until_date:
        try:
            start = datetime.strptime(since_date, '%Y-%m-%d')
            end = datetime.strptime(until_date, '%Y-%m-%d')
            expected_months = set()
            current = start.replace(day=1)
            while current <= end:
                expected_months.add(current.strftime('%Y-%m'))
                if current.month == 12:
                    current = current.replace(year=current.year + 1, month=1)
                else:
                    current = current.replace(month=current.month + 1)
            
            actual_months = set(monthly_commits.keys())
            if actual_months == expected_months:
                stability_score = min(100, stability_score + 10)
        except ValueError:
            pass
    
    stability_score = max(0, min(100, stability_score))
    
    # 3. è§£å†³é—®é¢˜èƒ½åŠ› (Problem Solving) - æ»¡åˆ†100
    problem_solving_score = (fix_commits / total_commits * 100) if total_commits > 0 else 0
    problem_solving_score = max(0, min(100, problem_solving_score))
    
    # 4. åŠŸèƒ½åˆ›æ–°åŠ› (Feature/Innovation) - æ»¡åˆ†100
    feature_score = (feat_commits / total_commits * 100) if total_commits > 0 else 0
    feature_score = max(0, min(100, feature_score))
    
    # 5. å¤šçº¿ä½œæˆ˜èƒ½åŠ› (Versatility) - æ»¡åˆ†100
    project_count = len(projects_set)
    project_score = min(50, project_count * 10)  # æœ€å¤š50åˆ†
    
    time_span_days = (end_date - start_date).days + 1
    time_span_score = min(50, (time_span_days / 365) * 50)  # æœ€å¤š50åˆ†
    
    versatility_score = project_score + time_span_score
    versatility_score = max(0, min(100, versatility_score))
    
    # è®¡ç®—æ€»ä½“è¯„åˆ†ï¼ˆå¹³å‡å€¼ï¼‰
    overall_score = (diligence_score + stability_score + problem_solving_score + 
                     feature_score + versatility_score) / 5
    
    # ç”Ÿæˆè¯¦ç»†åˆ†ææ–‡æœ¬
    def generate_analysis_text():
        analysis = {}
        
        # ä»£ç è´¨é‡è¯„ä¼°ï¼ˆåŸºäºæäº¤é¢‘ç‡ã€ç¨³å®šæ€§ã€ä»£ç è¡Œæ•°ç­‰ï¼‰
        code_quality_score = (diligence_score * 0.3 + stability_score * 0.3 + 
                             min(100, actual_frequency * 20) * 0.2 + 
                             min(100, project_count * 10) * 0.2)
        code_quality_analysis = f"åŸºäºæäº¤é¢‘ç‡({actual_frequency:.2f}æ¬¡/å¤©)ã€æ´»è·ƒå¤©æ•°({active_days}å¤©)å’Œé¡¹ç›®å‚ä¸åº¦({project_count}ä¸ªé¡¹ç›®)çš„ç»¼åˆè¯„ä¼°ã€‚"
        if actual_frequency > 2:
            code_quality_analysis += "æäº¤é¢‘ç‡è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å¼€å‘ä¹ æƒ¯ã€‚"
        if active_days / total_days > 0.5:
            code_quality_analysis += "æ´»è·ƒå¤©æ•°å æ¯”é«˜ï¼Œå·¥ä½œæŒç»­æ€§è‰¯å¥½ã€‚"
        
        analysis['code_quality'] = {
            'score': round(code_quality_score, 2),
            'analysis': code_quality_analysis,
            'strengths': [
                f"æ´»è·ƒå¤©æ•°: {active_days} å¤©" if active_days > 0 else "éœ€è¦æé«˜æ´»è·ƒåº¦",
                f"æäº¤é¢‘ç‡: {actual_frequency:.2f} æ¬¡/å¤©" if actual_frequency > 0 else "æäº¤é¢‘ç‡è¾ƒä½",
                f"æ¶‰åŠé¡¹ç›®: {project_count} ä¸ª" if project_count > 0 else "é¡¹ç›®å‚ä¸åº¦è¾ƒä½"
            ],
            'improvements': [
                "å»ºè®®ä¿æŒç¨³å®šçš„æäº¤é¢‘ç‡" if actual_frequency < 1 else "æäº¤é¢‘ç‡è‰¯å¥½",
                "å»ºè®®æé«˜ä»£ç æäº¤çš„æŒç»­æ€§" if active_days / total_days < 0.3 else "å·¥ä½œæŒç»­æ€§è‰¯å¥½"
            ]
        }
        
        # å·¥ä½œæ¨¡å¼åˆ†æ
        work_pattern_analysis = f"å·¥ä½œæ¨¡å¼åˆ†æï¼šæ´»è·ƒå¤©æ•°å æ¯” {active_days/total_days*100:.1f}%ï¼Œ"
        if len(monthly_commits) > 0:
            work_pattern_analysis += f"æ¶‰åŠ {len(monthly_commits)} ä¸ªæœˆï¼Œå¹³å‡æ¯æœˆ {mean_commits:.1f} æ¬¡æäº¤ã€‚"
        if cv < 0.5:
            work_pattern_analysis += "æäº¤åˆ†å¸ƒéå¸¸å‡åŒ€ï¼Œå·¥ä½œèŠ‚å¥ç¨³å®šã€‚"
        elif cv < 1.0:
            work_pattern_analysis += "æäº¤åˆ†å¸ƒè¾ƒä¸ºå‡åŒ€ï¼Œå·¥ä½œèŠ‚å¥è¾ƒç¨³å®šã€‚"
        else:
            work_pattern_analysis += "æäº¤åˆ†å¸ƒæ³¢åŠ¨è¾ƒå¤§ï¼Œå»ºè®®ä¿æŒæ›´ç¨³å®šçš„å·¥ä½œèŠ‚å¥ã€‚"
        
        analysis['work_pattern'] = {
            'score': round(stability_score, 2),
            'analysis': work_pattern_analysis,
            'strengths': [
                f"ç¨³å®šæ€§ç³»æ•°: {cv:.3f}" if cv > 0 else "æäº¤éå¸¸ç¨³å®š",
                f"æœˆåº¦åˆ†å¸ƒ: {len(monthly_commits)} ä¸ªæœˆæœ‰æäº¤" if len(monthly_commits) > 0 else "éœ€è¦æé«˜æœˆåº¦åˆ†å¸ƒ"
            ],
            'improvements': [
                "å»ºè®®ä¿æŒæ¯æœˆéƒ½æœ‰æäº¤è®°å½•" if len(monthly_commits) < 3 else "æœˆåº¦åˆ†å¸ƒè‰¯å¥½",
                "å»ºè®®å‡å°‘æäº¤æ•°é‡çš„æ³¢åŠ¨" if cv > 1.0 else "æäº¤åˆ†å¸ƒç¨³å®š"
            ]
        }
        
        # æŠ€æœ¯æ ˆè¯„ä¼°ï¼ˆåŸºäºé¡¹ç›®æ•°é‡å’Œæäº¤ç±»å‹ï¼‰
        tech_stack_analysis = f"æŠ€æœ¯æ ˆè¯„ä¼°ï¼šå‚ä¸ {project_count} ä¸ªé¡¹ç›®ï¼Œ"
        if project_count > 5:
            tech_stack_analysis += "é¡¹ç›®å‚ä¸åº¦é«˜ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å¤šé¡¹ç›®åä½œèƒ½åŠ›ã€‚"
        elif project_count > 2:
            tech_stack_analysis += "é¡¹ç›®å‚ä¸åº¦ä¸­ç­‰ï¼Œå»ºè®®æ‰©å±•é¡¹ç›®èŒƒå›´ã€‚"
        else:
            tech_stack_analysis += "é¡¹ç›®å‚ä¸åº¦è¾ƒä½ï¼Œå»ºè®®å¢åŠ é¡¹ç›®å‚ä¸ã€‚"
        
        analysis['tech_stack'] = {
            'score': round(min(100, project_count * 15 + min(50, time_span_days / 365 * 50)), 2),
            'analysis': tech_stack_analysis,
            'strengths': [
                f"é¡¹ç›®æ•°é‡: {project_count} ä¸ª",
                f"æ—¶é—´è·¨åº¦: {time_span_days} å¤©"
            ],
            'improvements': [
                "å»ºè®®å‚ä¸æ›´å¤šä¸åŒç±»å‹çš„é¡¹ç›®" if project_count < 3 else "é¡¹ç›®å‚ä¸åº¦è‰¯å¥½",
                "å»ºè®®ä¿æŒé•¿æœŸçš„é¡¹ç›®å‚ä¸" if time_span_days < 90 else "é¡¹ç›®å‚ä¸æ—¶é—´å……è¶³"
            ]
        }
        
        # é—®é¢˜è§£å†³èƒ½åŠ›
        problem_solving_analysis = f"é—®é¢˜è§£å†³èƒ½åŠ›ï¼šä¿®å¤ç±»æäº¤å æ¯” {fix_commits/total_commits*100:.1f}% ({fix_commits}/{total_commits})ã€‚"
        if fix_commits / total_commits > 0.3:
            problem_solving_analysis += "ä¿®å¤ç±»æäº¤å æ¯”è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„é—®é¢˜è§£å†³èƒ½åŠ›ã€‚"
        elif fix_commits / total_commits > 0.1:
            problem_solving_analysis += "ä¿®å¤ç±»æäº¤å æ¯”ä¸­ç­‰ï¼Œé—®é¢˜è§£å†³èƒ½åŠ›è‰¯å¥½ã€‚"
        else:
            problem_solving_analysis += "ä¿®å¤ç±»æäº¤å æ¯”è¾ƒä½ï¼Œå»ºè®®æé«˜é—®é¢˜è§£å†³èƒ½åŠ›ã€‚"
        
        analysis['problem_solving'] = {
            'score': round(problem_solving_score, 2),
            'analysis': problem_solving_analysis,
            'strengths': [
                f"ä¿®å¤ç±»æäº¤: {fix_commits} æ¬¡",
                f"ä¿®å¤å æ¯”: {fix_commits/total_commits*100:.1f}%" if total_commits > 0 else "æ— ä¿®å¤è®°å½•"
            ],
            'improvements': [
                "å»ºè®®æé«˜bugä¿®å¤çš„åŠæ—¶æ€§" if fix_commits / total_commits < 0.1 else "é—®é¢˜è§£å†³èƒ½åŠ›è‰¯å¥½",
                "å»ºè®®è®°å½•æ›´è¯¦ç»†çš„ä¿®å¤ä¿¡æ¯" if fix_commits > 0 else "å»ºè®®å¢åŠ é—®é¢˜ä¿®å¤çš„æäº¤"
            ]
        }
        
        # åˆ›æ–°æ€§åˆ†æ
        innovation_analysis = f"åˆ›æ–°æ€§åˆ†æï¼šåŠŸèƒ½å¼€å‘ç±»æäº¤å æ¯” {feat_commits/total_commits*100:.1f}% ({feat_commits}/{total_commits})ã€‚"
        if feat_commits / total_commits > 0.4:
            innovation_analysis += "åŠŸèƒ½å¼€å‘ç±»æäº¤å æ¯”è¾ƒé«˜ï¼Œæ˜¾ç¤ºå‡ºè‰¯å¥½çš„åˆ›æ–°èƒ½åŠ›å’ŒåŠŸèƒ½å¼€å‘èƒ½åŠ›ã€‚"
        elif feat_commits / total_commits > 0.2:
            innovation_analysis += "åŠŸèƒ½å¼€å‘ç±»æäº¤å æ¯”ä¸­ç­‰ï¼Œåˆ›æ–°èƒ½åŠ›è‰¯å¥½ã€‚"
        else:
            innovation_analysis += "åŠŸèƒ½å¼€å‘ç±»æäº¤å æ¯”è¾ƒä½ï¼Œå»ºè®®å¢åŠ æ–°åŠŸèƒ½å¼€å‘ã€‚"
        
        analysis['innovation'] = {
            'score': round(feature_score, 2),
            'analysis': innovation_analysis,
            'strengths': [
                f"åŠŸèƒ½å¼€å‘æäº¤: {feat_commits} æ¬¡",
                f"åŠŸèƒ½å æ¯”: {feat_commits/total_commits*100:.1f}%" if total_commits > 0 else "æ— åŠŸèƒ½å¼€å‘è®°å½•"
            ],
            'improvements': [
                "å»ºè®®å¢åŠ æ–°åŠŸèƒ½çš„å¼€å‘" if feat_commits / total_commits < 0.2 else "åŠŸèƒ½å¼€å‘èƒ½åŠ›è‰¯å¥½",
                "å»ºè®®è®°å½•æ›´è¯¦ç»†çš„åŠŸèƒ½å¼€å‘ä¿¡æ¯" if feat_commits > 0 else "å»ºè®®å¢åŠ åŠŸèƒ½å¼€å‘çš„æäº¤"
            ]
        }
        
        # å›¢é˜Ÿåä½œ
        collaboration_analysis = f"å›¢é˜Ÿåä½œï¼šåŒæ—¶ç»´æŠ¤ {project_count} ä¸ªé¡¹ç›®ï¼Œæ—¶é—´è·¨åº¦ {time_span_days} å¤©ã€‚"
        if project_count > 3 and time_span_days > 180:
            collaboration_analysis += "å¤šé¡¹ç›®åä½œèƒ½åŠ›å¼ºï¼Œèƒ½å¤ŸåŒæ—¶ç»´æŠ¤å¤šä¸ªé¡¹ç›®å¹¶ä¿æŒé•¿æœŸå‚ä¸ã€‚"
        elif project_count > 1:
            collaboration_analysis += "å…·å¤‡å¤šé¡¹ç›®åä½œèƒ½åŠ›ï¼Œå»ºè®®ä¿æŒé•¿æœŸå‚ä¸ã€‚"
        else:
            collaboration_analysis += "å»ºè®®å¢åŠ é¡¹ç›®å‚ä¸ï¼Œæé«˜å›¢é˜Ÿåä½œèƒ½åŠ›ã€‚"
        
        analysis['collaboration'] = {
            'score': round(versatility_score, 2),
            'analysis': collaboration_analysis,
            'strengths': [
                f"é¡¹ç›®æ•°é‡: {project_count} ä¸ª",
                f"æ—¶é—´è·¨åº¦: {time_span_days} å¤©",
                f"æ´»è·ƒå¤©æ•°: {active_days} å¤©"
            ],
            'improvements': [
                "å»ºè®®å‚ä¸æ›´å¤šé¡¹ç›®" if project_count < 2 else "é¡¹ç›®å‚ä¸åº¦è‰¯å¥½",
                "å»ºè®®ä¿æŒé•¿æœŸçš„é¡¹ç›®å‚ä¸" if time_span_days < 90 else "é¡¹ç›®å‚ä¸æ—¶é—´å……è¶³"
            ]
        }
        
        return analysis
    
    detailed_analysis = generate_analysis_text()
    
    return {
        'diligence': {
            'score': round(diligence_score, 2),
            'active_days': active_days,
            'total_days': total_days,
            'total_commits': total_commits,
            'frequency': round(actual_frequency, 2)
        },
        'stability': {
            'score': round(stability_score, 2),
            'monthly_commits': dict(monthly_commits),
            'cv': round(cv, 3) if len(monthly_commits) > 1 and mean_commits > 0 else 0
        },
        'problem_solving': {
            'score': round(problem_solving_score, 2),
            'fix_commits': fix_commits,
            'total_commits': total_commits,
            'ratio': round(fix_commits / total_commits, 3) if total_commits > 0 else 0
        },
        'feature_innovation': {
            'score': round(feature_score, 2),
            'feat_commits': feat_commits,
            'total_commits': total_commits,
            'ratio': round(feat_commits / total_commits, 3) if total_commits > 0 else 0
        },
        'versatility': {
            'score': round(versatility_score, 2),
            'project_count': project_count,
            'time_span_days': time_span_days
        },
        'overall': round(overall_score, 2),
        'detailed_analysis': detailed_analysis  # æ–°å¢è¯¦ç»†åˆ†æ
    }


def generate_statistics_report(all_results, author_name, since_date=None, until_date=None):
    """
    ç”ŸæˆåŒ…å«ç»Ÿè®¡å’Œè¯„åˆ†çš„æŠ¥å‘Š
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: Markdown æ ¼å¼çš„ç»Ÿè®¡æŠ¥å‘Šå†…å®¹
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {author_name} - ä»£ç ç»Ÿè®¡ä¸è¯„åˆ†æŠ¥å‘Š\n")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    
    # æ—¥æœŸèŒƒå›´
    if since_date and until_date:
        lines.append(f"**ç»Ÿè®¡æ—¶é—´èŒƒå›´**: {since_date} è‡³ {until_date}\n")
    elif since_date:
        lines.append(f"**èµ·å§‹æ—¥æœŸ**: {since_date}\n")
    elif until_date:
        lines.append(f"**ç»“æŸæ—¥æœŸ**: {until_date}\n")
    
    lines.append("\n---\n\n")
    
    # è®¡ç®—ä»£ç ç»Ÿè®¡ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼Œæ·»åŠ å¼‚å¸¸å¤„ç†ï¼‰
    code_stats = None
    try:
        logger.info("æ­£åœ¨è®¡ç®—ä»£ç è¡Œæ•°ç»Ÿè®¡ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        code_stats = calculate_code_statistics(all_results, since_date, until_date)
        logger.info("ä»£ç è¡Œæ•°ç»Ÿè®¡è®¡ç®—å®Œæˆ")
    except Exception as e:
        logger.warning(f"è®¡ç®—ä»£ç è¡Œæ•°ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}")
        logger.warning("å°†è·³è¿‡ä»£ç è¡Œæ•°ç»Ÿè®¡ï¼Œç»§ç»­ç”Ÿæˆè¯„åˆ†æŠ¥å‘Š")
        # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„ç»Ÿè®¡ç»“æœ
        total_commits = sum(len(result['commits']) for result in all_results.values())
        code_stats = {
            'total_additions': 0,
            'total_deletions': 0,
            'net_lines': 0,
            'total_commits': total_commits,
            'commits_with_stats': 0,
            'avg_lines_per_commit': 0,
            'stats_availability': 0
        }
    
    # ä»£ç è¡Œæ•°ç»Ÿè®¡
    lines.append("## ğŸ“Š ä»£ç è¡Œæ•°ç»Ÿè®¡\n\n")
    if code_stats['commits_with_stats'] > 0:
        lines.append(f"- **æ€»æ–°å¢è¡Œæ•°**: {code_stats['total_additions']:,}\n")
        lines.append(f"- **æ€»åˆ é™¤è¡Œæ•°**: {code_stats['total_deletions']:,}\n")
        lines.append(f"- **å‡€å¢è¡Œæ•°**: {code_stats['net_lines']:,}\n")
        lines.append(f"- **æ€»æäº¤æ•°**: {code_stats['total_commits']}\n")
        lines.append(f"- **æœ‰ç»Ÿè®¡ä¿¡æ¯çš„æäº¤æ•°**: {code_stats['commits_with_stats']}\n")
        lines.append(f"- **å¹³å‡æ¯æ¬¡æäº¤ä»£ç è¡Œæ•°**: {code_stats['avg_lines_per_commit']}\n")
        lines.append(f"- **ç»Ÿè®¡ä¿¡æ¯å¯ç”¨ç‡**: {code_stats['stats_availability']:.1%}\n")
    else:
        lines.append(f"- **æ€»æäº¤æ•°**: {code_stats['total_commits']}\n")
        lines.append("- **ä»£ç è¡Œæ•°ç»Ÿè®¡**: æš‚ä¸å¯ç”¨ï¼ˆéœ€è¦APIæƒé™æˆ–APIè°ƒç”¨å¤±è´¥ï¼‰\n")
        lines.append("- **æç¤º**: ä»£ç è¡Œæ•°ç»Ÿè®¡éœ€è¦é¢å¤–çš„APIè°ƒç”¨ï¼Œå¯èƒ½å› ä¸ºæƒé™ä¸è¶³æˆ–ç½‘ç»œé—®é¢˜è€Œæ— æ³•è·å–\n")
    lines.append("\n---\n\n")
    
    # è®¡ç®—è¯„åˆ†
    try:
        logger.info("æ­£åœ¨è®¡ç®—å¤šç»´åº¦è¯„åˆ†...")
        scores = calculate_scores(all_results, since_date, until_date)
        logger.info("å¤šç»´åº¦è¯„åˆ†è®¡ç®—å®Œæˆ")
    except Exception as e:
        logger.error(f"è®¡ç®—è¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise  # è¯„åˆ†æ˜¯æ ¸å¿ƒåŠŸèƒ½ï¼Œå¦‚æœå¤±è´¥åº”è¯¥æŠ›å‡ºå¼‚å¸¸
    
    # è¯„åˆ†è¯¦æƒ…
    lines.append("## ğŸ¯ å¤šç»´åº¦è¯„åˆ†\n\n")
    lines.append(f"**æ€»ä½“è¯„åˆ†**: {scores['overall']:.2f} / 100\n\n")
    
    # å‹¤å¥‹åº¦
    lines.append(f"### 1. å‹¤å¥‹åº¦ (Diligence): {scores['diligence']['score']:.2f} / 100\n\n")
    lines.append(f"- æ´»è·ƒå¤©æ•°: {scores['diligence']['active_days']} å¤© / {scores['diligence']['total_days']} å¤©\n")
    lines.append(f"- æ€»æäº¤æ•°: {scores['diligence']['total_commits']} æ¬¡\n")
    lines.append(f"- å¹³å‡æäº¤é¢‘ç‡: {scores['diligence']['frequency']:.2f} æ¬¡/å¤©\n")
    lines.append(f"- è¯„åˆ†è¯´æ˜: åŸºäºæ´»è·ƒå¤©æ•°å’Œæäº¤é¢‘ç‡ç»¼åˆè¯„ä¼°\n\n")
    
    # ç¨³å®šæ€§
    lines.append(f"### 2. ç¨³å®šæ€§ (Stability): {scores['stability']['score']:.2f} / 100\n\n")
    lines.append(f"- æœˆåº¦æäº¤åˆ†å¸ƒ: {len(scores['stability']['monthly_commits'])} ä¸ªæœˆæœ‰æäº¤\n")
    if scores['stability']['cv'] > 0:
        lines.append(f"- ç¦»æ•£ç³»æ•°: {scores['stability']['cv']:.3f}\n")
    lines.append(f"- è¯„åˆ†è¯´æ˜: åŸºäºæ¯æœˆæäº¤åˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦è¯„ä¼°ï¼Œç¦»æ•£ç³»æ•°è¶Šå°è¶Šç¨³å®š\n\n")
    
    # è§£å†³é—®é¢˜èƒ½åŠ›
    lines.append(f"### 3. è§£å†³é—®é¢˜èƒ½åŠ› (Problem Solving): {scores['problem_solving']['score']:.2f} / 100\n\n")
    lines.append(f"- ä¿®å¤ç±»æäº¤: {scores['problem_solving']['fix_commits']} æ¬¡\n")
    lines.append(f"- ä¿®å¤ç±»æäº¤å æ¯”: {scores['problem_solving']['ratio']:.1%}\n")
    lines.append(f"- è¯„åˆ†è¯´æ˜: åŸºäºä¿®å¤ç±»æäº¤ï¼ˆfix/bug/ä¿®å¤ç­‰å…³é”®è¯ï¼‰çš„å æ¯”\n\n")
    
    # åŠŸèƒ½åˆ›æ–°åŠ›
    lines.append(f"### 4. åŠŸèƒ½åˆ›æ–°åŠ› (Feature/Innovation): {scores['feature_innovation']['score']:.2f} / 100\n\n")
    lines.append(f"- åŠŸèƒ½ç±»æäº¤: {scores['feature_innovation']['feat_commits']} æ¬¡\n")
    lines.append(f"- åŠŸèƒ½ç±»æäº¤å æ¯”: {scores['feature_innovation']['ratio']:.1%}\n")
    lines.append(f"- è¯„åˆ†è¯´æ˜: åŸºäºæ–°åŠŸèƒ½å¼€å‘æäº¤ï¼ˆfeat/add/æ–°å¢ç­‰å…³é”®è¯ï¼‰çš„å æ¯”\n\n")
    
    # å¤šçº¿ä½œæˆ˜èƒ½åŠ›
    lines.append(f"### 5. å¤šçº¿ä½œæˆ˜èƒ½åŠ› (Versatility): {scores['versatility']['score']:.2f} / 100\n\n")
    lines.append(f"- æ¶‰åŠé¡¹ç›®æ•°: {scores['versatility']['project_count']} ä¸ª\n")
    lines.append(f"- æ—¶é—´è·¨åº¦: {scores['versatility']['time_span_days']} å¤©\n")
    lines.append(f"- è¯„åˆ†è¯´æ˜: åŸºäºåŒæ—¶ç»´æŠ¤çš„é¡¹ç›®æ•°é‡å’Œæ—¶é—´è·¨åº¦\n\n")
    
    lines.append("---\n\n")
    
    # è¯„åˆ†å¯è§†åŒ–ï¼ˆä½¿ç”¨è¿›åº¦æ¡ï¼‰
    lines.append("## ğŸ“ˆ è¯„åˆ†å¯è§†åŒ–\n\n")
    
    def progress_bar(score, max_score=100, length=20):
        """ç”Ÿæˆè¿›åº¦æ¡"""
        filled = int(score / max_score * length)
        bar = 'â–ˆ' * filled + 'â–‘' * (length - filled)
        return f"`{bar}` {score:.1f}%"
    
    lines.append(f"- **å‹¤å¥‹åº¦**: {progress_bar(scores['diligence']['score'])}\n")
    lines.append(f"- **ç¨³å®šæ€§**: {progress_bar(scores['stability']['score'])}\n")
    lines.append(f"- **è§£å†³é—®é¢˜èƒ½åŠ›**: {progress_bar(scores['problem_solving']['score'])}\n")
    lines.append(f"- **åŠŸèƒ½åˆ›æ–°åŠ›**: {progress_bar(scores['feature_innovation']['score'])}\n")
    lines.append(f"- **å¤šçº¿ä½œæˆ˜èƒ½åŠ›**: {progress_bar(scores['versatility']['score'])}\n")
    lines.append(f"- **æ€»ä½“è¯„åˆ†**: {progress_bar(scores['overall'])}\n")
    
    lines.append("\n---\n\n")
    
    return ''.join(lines)


def generate_all_reports(all_results, author_name, output_dir, since_date=None, until_date=None, 
                         generate_statistics=True, generate_daily=True, generate_html=True, 
                         generate_png=True, logger_func=None):
    """
    æ‰¹é‡ç”Ÿæˆæ‰€æœ‰æ ¼å¼çš„æŠ¥å‘Š
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        output_dir: è¾“å‡ºç›®å½•
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
        generate_statistics: æ˜¯å¦ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        generate_daily: æ˜¯å¦ç”Ÿæˆå¼€å‘æ—¥æŠ¥
        generate_html: æ˜¯å¦ç”ŸæˆHTMLæ ¼å¼
        generate_png: æ˜¯å¦ç”ŸæˆPNGå›¾ç‰‡
        logger_func: æ—¥å¿—è¾“å‡ºå‡½æ•°ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        dict: ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    import os
    from pathlib import Path
    from datetime import datetime
    
    if logger_func:
        log = logger_func
    else:
        log = logger.info
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ç¡®å®šæ–‡ä»¶å‰ç¼€
    if since_date and until_date and since_date == until_date:
        date_prefix = since_date
    else:
        date_prefix = datetime.now().strftime('%Y-%m-%d')
    
    generated_files = {}
    
    # 1. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    if generate_statistics:
        try:
            log("æ­£åœ¨ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š...")
            stats_content = generate_statistics_report(
                all_results, author_name, since_date, until_date
            )
            stats_file = output_path / f"{date_prefix}_statistics.md"
            with open(stats_file, 'w', encoding='utf-8') as f:
                f.write(stats_content)
            generated_files['statistics'] = str(stats_file)
            log(f"âœ“ ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜: {stats_file}")
        except Exception as e:
            log(f"âœ— ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šå¤±è´¥: {str(e)}")
            generated_files['statistics'] = None
    
    # 2. ç”Ÿæˆå¼€å‘æ—¥æŠ¥
    daily_file = None
    if generate_daily:
        try:
            log("æ­£åœ¨ç”Ÿæˆå¼€å‘æ—¥æŠ¥...")
            daily_content = generate_daily_report(
                all_results, author_name, since_date, until_date
            )
            daily_file = output_path / f"{date_prefix}_daily_report.md"
            with open(daily_file, 'w', encoding='utf-8') as f:
                f.write(daily_content)
            generated_files['daily_report'] = str(daily_file)
            log(f"âœ“ å¼€å‘æ—¥æŠ¥å·²ä¿å­˜: {daily_file}")
        except Exception as e:
            log(f"âœ— ç”Ÿæˆå¼€å‘æ—¥æŠ¥å¤±è´¥: {str(e)}")
            generated_files['daily_report'] = None
    
    # 3. ç”ŸæˆHTMLæ ¼å¼ï¼ˆéœ€è¦åŸºäºæ—¥æŠ¥ï¼‰
    html_file = None
    if generate_html and daily_file and daily_file.exists():
        try:
            log("æ­£åœ¨ç”ŸæˆHTMLæ ¼å¼...")
            # å°è¯•å¯¼å…¥ generate_report_image æ¨¡å—
            try:
                from generate_report_image import parse_daily_report, generate_html_report
                data = parse_daily_report(str(daily_file))
                html_file = output_path / f"{date_prefix}_daily_report.html"
                generate_html_report(data, str(html_file))
                generated_files['html'] = str(html_file)
                log(f"âœ“ HTMLæ–‡ä»¶å·²ä¿å­˜: {html_file}")
            except ImportError:
                log("âš  æ— æ³•å¯¼å…¥ generate_report_image æ¨¡å—ï¼Œè·³è¿‡HTMLç”Ÿæˆ")
                generated_files['html'] = None
            except Exception as e:
                log(f"âœ— ç”ŸæˆHTMLå¤±è´¥: {str(e)}")
                generated_files['html'] = None
        except Exception as e:
            log(f"âœ— ç”ŸæˆHTMLå¤±è´¥: {str(e)}")
            generated_files['html'] = None
    
    # 4. ç”ŸæˆPNGå›¾ç‰‡ï¼ˆéœ€è¦åŸºäºHTMLï¼‰
    if generate_png and html_file and html_file.exists():
        try:
            log("æ­£åœ¨ç”ŸæˆPNGå›¾ç‰‡...")
            try:
                from generate_report_image import html_to_image_chrome
                png_file = output_path / f"{date_prefix}_daily_report.png"
                if html_to_image_chrome(str(html_file), str(png_file)):
                    generated_files['png'] = str(png_file)
                    log(f"âœ“ PNGå›¾ç‰‡å·²ä¿å­˜: {png_file}")
                else:
                    log("âš  PNGå›¾ç‰‡ç”Ÿæˆå¤±è´¥ï¼ˆå¯èƒ½éœ€è¦Chromeæµè§ˆå™¨ï¼‰")
                    generated_files['png'] = None
            except ImportError:
                log("âš  æ— æ³•å¯¼å…¥ generate_report_image æ¨¡å—ï¼Œè·³è¿‡PNGç”Ÿæˆ")
                generated_files['png'] = None
            except Exception as e:
                log(f"âœ— ç”ŸæˆPNGå¤±è´¥: {str(e)}")
                generated_files['png'] = None
        except Exception as e:
            log(f"âœ— ç”ŸæˆPNGå¤±è´¥: {str(e)}")
            generated_files['png'] = None
    
    log(f"æ‰¹é‡ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len([f for f in generated_files.values() if f])} ä¸ªæ–‡ä»¶")
    return generated_files


def analyze_with_ai(all_results, author_name, ai_config, since_date=None, until_date=None):
    """
    æ”¶é›†æäº¤æ•°æ®å¹¶ä½¿ç”¨AIè¿›è¡Œåˆ†æ
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        ai_config: AIé…ç½®å­—å…¸
            - service: 'openai', 'anthropic' æˆ– 'gemini'
            - api_key: APIå¯†é’¥
            - model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        dict: AIåˆ†æç»“æœ
    """
    # ç¡®ä¿ datetime å·²å¯¼å…¥ï¼ˆé¿å…ä½œç”¨åŸŸé—®é¢˜ï¼‰
    from datetime import datetime
    
    try:
        from ai_analysis import analyze_with_ai as call_ai_service
    except ImportError:
        logger.error("æ— æ³•å¯¼å…¥ ai_analysis æ¨¡å—")
        raise
    
    # æ”¶é›†æäº¤æ•°æ®
    commits_data = {
        'total_commits': 0,
        'active_days': 0,
        'projects': [],
        'commit_messages': [],
        'time_distribution': {},
        'code_stats': {}
    }
    
    all_dates = set()
    all_commit_messages = []
    projects_set = set()
    
    # è®¡ç®—ä»£ç ç»Ÿè®¡
    try:
        code_stats = calculate_code_statistics(all_results, since_date, until_date)
        commits_data['code_stats'] = code_stats
    except Exception as e:
        logger.warning(f"è®¡ç®—ä»£ç ç»Ÿè®¡å¤±è´¥: {str(e)}")
        commits_data['code_stats'] = {
            'total_additions': 0,
            'total_deletions': 0
        }
    
    # æ”¶é›†æäº¤ä¿¡æ¯
    for project_path, result in all_results.items():
        projects_set.add(project_path)
        commits = result['commits']
        commits_data['total_commits'] += len(commits)
        
        for commit in commits:
            # æ”¶é›†commit message
            if commit.message:
                all_commit_messages.append(commit.message[:200])  # é™åˆ¶é•¿åº¦
            
            # æ”¶é›†æ—¥æœŸ
            commit_date = commit.committed_date
            if isinstance(commit_date, str):
                date_obj = datetime.fromisoformat(commit_date.replace('Z', '+00:00'))
            else:
                date_obj = commit_date
            date_str = date_obj.strftime('%Y-%m-%d')
            all_dates.add(date_str)
            
            # æ”¶é›†æ—¶é—´åˆ†å¸ƒï¼ˆæŒ‰æœˆï¼‰
            month_key = date_obj.strftime('%Y-%m')
            commits_data['time_distribution'][month_key] = commits_data['time_distribution'].get(month_key, 0) + 1
    
    commits_data['active_days'] = len(all_dates)
    commits_data['projects'] = list(projects_set)
    commits_data['commit_messages'] = all_commit_messages[:50]  # æœ€å¤š50æ¡
    
    # è°ƒç”¨AIåˆ†æï¼ˆå¸¦è¶…æ—¶ï¼‰
    timeout = 120  # é»˜è®¤120ç§’è¶…æ—¶
    logger.info(f"æ­£åœ¨è°ƒç”¨AIæœåŠ¡è¿›è¡Œåˆ†æï¼ˆè¶…æ—¶æ—¶é—´: {timeout}ç§’ï¼‰...")
    try:
        analysis_result = call_ai_service(commits_data, ai_config, timeout=timeout)
        logger.info("AIåˆ†æå®Œæˆ")
        # åœ¨ç»“æœä¸­æ·»åŠ AIæœåŠ¡ä¿¡æ¯
        analysis_result['ai_service'] = ai_config.get('service', 'unknown')
        analysis_result['ai_model'] = ai_config.get('model', 'unknown')
        return analysis_result
    except TimeoutError as e:
        logger.error(f"AIåˆ†æè¶…æ—¶: {str(e)}")
        raise
    except ValueError as e:
        # APIå¯†é’¥é”™è¯¯ç­‰
        logger.error(f"AIåˆ†æå¤±è´¥ï¼ˆå¯èƒ½æ˜¯APIå¯†é’¥é—®é¢˜ï¼‰: {str(e)}")
        raise
    except ConnectionError as e:
        # ç½‘ç»œé”™è¯¯
        logger.error(f"AIåˆ†æå¤±è´¥ï¼ˆç½‘ç»œè¿æ¥é—®é¢˜ï¼‰: {str(e)}")
        raise
    except Exception as e:
        logger.error(f"AIåˆ†æå¤±è´¥: {str(e)}")
        raise


def generate_local_analysis_report(all_results, author_name, since_date=None, until_date=None):
    """
    ä½¿ç”¨æœ¬åœ°è¯„ä»·é€»è¾‘ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼ˆå½“æ²¡æœ‰AIå¯†é’¥æ—¶ä½¿ç”¨ï¼‰
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: Markdownæ ¼å¼çš„æœ¬åœ°åˆ†ææŠ¥å‘Š
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {author_name} - æœ¬åœ°æ™ºèƒ½åˆ†ææŠ¥å‘Š\n")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    lines.append(f"**åˆ†ææ–¹å¼**: ğŸ“Š æœ¬åœ°è¯„ä»·é€»è¾‘ï¼ˆåŸºäºç»Ÿè®¡æ•°æ®å’Œè§„åˆ™ç®—æ³•ï¼Œæ— éœ€AIæœåŠ¡ï¼‰\n")
    
    if since_date and until_date:
        lines.append(f"**åˆ†ææ—¶é—´èŒƒå›´**: {since_date} è‡³ {until_date}\n")
    elif since_date:
        lines.append(f"**èµ·å§‹æ—¥æœŸ**: {since_date}\n")
    elif until_date:
        lines.append(f"**ç»“æŸæ—¥æœŸ**: {until_date}\n")
    
    lines.append("\n---\n\n")
    
    # è®¡ç®—è¯„åˆ†å’Œè¯¦ç»†åˆ†æ
    try:
        scores = calculate_scores(all_results, since_date, until_date)
        detailed_analysis = scores.get('detailed_analysis', {})
        
        # æ‰§è¡Œæ‘˜è¦
        lines.append("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n")
        overall_score = scores.get('overall', 0)
        lines.append(f"**æ€»ä½“è¯„åˆ†**: {overall_score:.1f} / 100\n\n")
        lines.append("**å„ç»´åº¦è¯„åˆ†**:\n")
        
        dimension_map = {
            'code_quality': 'ä»£ç è´¨é‡',
            'work_pattern': 'å·¥ä½œæ¨¡å¼',
            'tech_stack': 'æŠ€æœ¯æ ˆ',
            'problem_solving': 'é—®é¢˜è§£å†³èƒ½åŠ›',
            'innovation': 'åˆ›æ–°æ€§',
            'collaboration': 'å›¢é˜Ÿåä½œ'
        }
        
        for dim_key, dim_name in dimension_map.items():
            if dim_key in detailed_analysis:
                score = detailed_analysis[dim_key].get('score', 0)
                lines.append(f"- {dim_name}: {score:.1f} / 100\n")
        
        lines.append("\n---\n\n")
        
        # è¯¦ç»†åˆ†æ
        lines.append("## ğŸ” è¯¦ç»†åˆ†æ\n\n")
        
        for dim_key, dim_name in dimension_map.items():
            if dim_key in detailed_analysis:
                dim_data = detailed_analysis[dim_key]
                score = dim_data.get('score', 0)
                
                lines.append(f"### {dim_name}: {score:.1f} / 100\n\n")
                
                # è¯¦ç»†åˆ†æ
                if 'analysis' in dim_data:
                    lines.append(f"**åˆ†æ**:\n{dim_data['analysis']}\n\n")
                
                # ä¼˜åŠ¿
                if 'strengths' in dim_data and dim_data['strengths']:
                    lines.append("**ä¼˜åŠ¿**:\n")
                    if isinstance(dim_data['strengths'], list):
                        for strength in dim_data['strengths']:
                            lines.append(f"- {strength}\n")
                    else:
                        lines.append(f"- {dim_data['strengths']}\n")
                    lines.append("\n")
                
                # æ”¹è¿›å»ºè®®
                if 'improvements' in dim_data and dim_data['improvements']:
                    lines.append("**æ”¹è¿›å»ºè®®**:\n")
                    if isinstance(dim_data['improvements'], list):
                        for improvement in dim_data['improvements']:
                            lines.append(f"- {improvement}\n")
                    else:
                        lines.append(f"- {dim_data['improvements']}\n")
                    lines.append("\n")
                
                lines.append("---\n\n")
        
        # åŸå§‹è¯„åˆ†æ•°æ®
        lines.append("## ğŸ“Š åŸå§‹è¯„åˆ†æ•°æ®\n\n")
        lines.append(f"- **å‹¤å¥‹åº¦**: {scores.get('diligence', {}).get('score', 0):.1f} / 100\n")
        lines.append(f"- **ç¨³å®šæ€§**: {scores.get('stability', {}).get('score', 0):.1f} / 100\n")
        lines.append(f"- **é—®é¢˜è§£å†³èƒ½åŠ›**: {scores.get('problem_solving', {}).get('score', 0):.1f} / 100\n")
        lines.append(f"- **åŠŸèƒ½åˆ›æ–°åŠ›**: {scores.get('feature_innovation', {}).get('score', 0):.1f} / 100\n")
        lines.append(f"- **å¤šçº¿ä½œæˆ˜èƒ½åŠ›**: {scores.get('versatility', {}).get('score', 0):.1f} / 100\n")
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæœ¬åœ°åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}")
        lines.append(f"**é”™è¯¯**: ç”Ÿæˆåˆ†ææŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}\n")
    
    lines.append("\n---\n\n")
    lines.append("**æ³¨**: æœ¬æŠ¥å‘Šä½¿ç”¨æœ¬åœ°è¯„ä»·é€»è¾‘ç”Ÿæˆï¼ŒåŸºäºç»Ÿè®¡æ•°æ®å’Œè§„åˆ™åˆ†æã€‚å¦‚éœ€æ›´æ·±å…¥çš„AIåˆ†æï¼Œè¯·é…ç½®AIæœåŠ¡ã€‚\n")
    
    return ''.join(lines)


def generate_ai_analysis_report(analysis_result, author_name, since_date=None, until_date=None):
    """
    ç”ŸæˆAIåˆ†ææŠ¥å‘Š
    
    Args:
        analysis_result: AIåˆ†æç»“æœå­—å…¸
        author_name: æäº¤è€…å§“å
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: Markdownæ ¼å¼çš„AIåˆ†ææŠ¥å‘Š
    """
    lines = []
    
    # æ ‡é¢˜
    lines.append(f"# {author_name} - AIæ™ºèƒ½åˆ†ææŠ¥å‘Š\n")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    lines.append(f"**åˆ†ææ–¹å¼**: ğŸ¤– AIæ™ºèƒ½åˆ†æï¼ˆä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ·±åº¦åˆ†æï¼‰\n")
    
    # ä»analysis_resultä¸­æå–AIæœåŠ¡ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'ai_service' in analysis_result:
        lines.append(f"**AIæœåŠ¡**: {analysis_result.get('ai_service', 'æœªçŸ¥')}\n")
    if 'ai_model' in analysis_result:
        lines.append(f"**AIæ¨¡å‹**: {analysis_result.get('ai_model', 'æœªçŸ¥')}\n")
    
    if since_date and until_date:
        lines.append(f"**åˆ†ææ—¶é—´èŒƒå›´**: {since_date} è‡³ {until_date}\n")
    elif since_date:
        lines.append(f"**èµ·å§‹æ—¥æœŸ**: {since_date}\n")
    elif until_date:
        lines.append(f"**ç»“æŸæ—¥æœŸ**: {until_date}\n")
    
    lines.append("\n---\n\n")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if 'error' in analysis_result:
        lines.append("## âš ï¸ åˆ†æé”™è¯¯\n\n")
        lines.append(f"AIåˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {analysis_result['error']}\n\n")
        if 'raw_response' in analysis_result:
            lines.append("### åŸå§‹å“åº”\n\n")
            lines.append(f"```\n{analysis_result['raw_response']}\n```\n")
        return ''.join(lines)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹å“åº”ä½†æ— æ³•è§£æï¼ˆè¿™ç§æƒ…å†µä¹Ÿåº”è¯¥æ˜¾ç¤ºåŸå§‹å“åº”ï¼‰
    if 'raw_response' in analysis_result and not any(
        dim in analysis_result and isinstance(analysis_result[dim], dict) 
        for dim in ['code_quality', 'work_pattern', 'tech_stack', 'problem_solving', 'innovation', 'collaboration']
    ):
        lines.append("## âš ï¸ è§£æè­¦å‘Š\n\n")
        lines.append("AIè¿”å›çš„å“åº”æ— æ³•è§£æä¸ºç»“æ„åŒ–JSONæ ¼å¼ï¼Œä»¥ä¸‹æ˜¯åŸå§‹å“åº”ï¼š\n\n")
        lines.append("### åŸå§‹å“åº”\n\n")
        lines.append(f"```\n{analysis_result['raw_response']}\n```\n\n")
        lines.append("**æç¤º**: è¿™å¯èƒ½æ˜¯ç”±äºAIè¿”å›çš„æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œæˆ–è€…å“åº”ä¸­åŒ…å«æ— æ³•è§£æçš„å†…å®¹ã€‚\n")
        return ''.join(lines)
    
    # æ‰§è¡Œæ‘˜è¦
    lines.append("## ğŸ“‹ æ‰§è¡Œæ‘˜è¦\n\n")
    
    # è®¡ç®—æ€»ä½“è¯„åˆ†
    dimensions = ['code_quality', 'work_pattern', 'tech_stack', 'problem_solving', 'innovation', 'collaboration']
    scores = []
    for dim in dimensions:
        if dim in analysis_result and isinstance(analysis_result[dim], dict):
            score = analysis_result[dim].get('score', 0)
            scores.append(score)
    
    if scores:
        overall_score = sum(scores) / len(scores)
        lines.append(f"**æ€»ä½“è¯„åˆ†**: {overall_score:.1f} / 100\n\n")
        lines.append("**å„ç»´åº¦è¯„åˆ†**:\n")
        for dim in dimensions:
            if dim in analysis_result and isinstance(analysis_result[dim], dict):
                score = analysis_result[dim].get('score', 0)
                dim_name = {
                    'code_quality': 'ä»£ç è´¨é‡',
                    'work_pattern': 'å·¥ä½œæ¨¡å¼',
                    'tech_stack': 'æŠ€æœ¯æ ˆ',
                    'problem_solving': 'é—®é¢˜è§£å†³èƒ½åŠ›',
                    'innovation': 'åˆ›æ–°æ€§',
                    'collaboration': 'å›¢é˜Ÿåä½œ'
                }.get(dim, dim)
                lines.append(f"- {dim_name}: {score:.1f} / 100\n")
        lines.append("\n")
    
    lines.append("---\n\n")
    
    # è¯¦ç»†åˆ†æ
    lines.append("## ğŸ” è¯¦ç»†åˆ†æ\n\n")
    
    dimension_names = {
        'code_quality': 'ä»£ç è´¨é‡è¯„ä¼°',
        'work_pattern': 'å·¥ä½œæ¨¡å¼åˆ†æ',
        'tech_stack': 'æŠ€æœ¯æ ˆè¯„ä¼°',
        'problem_solving': 'é—®é¢˜è§£å†³èƒ½åŠ›',
        'innovation': 'åˆ›æ–°æ€§åˆ†æ',
        'collaboration': 'å›¢é˜Ÿåä½œ'
    }
    
    for dim in dimensions:
        if dim in analysis_result and isinstance(analysis_result[dim], dict):
            dim_data = analysis_result[dim]
            dim_name = dimension_names.get(dim, dim)
            score = dim_data.get('score', 0)
            
            lines.append(f"### {dim_name}: {score:.1f} / 100\n\n")
            
            # è¯¦ç»†åˆ†æ
            if 'analysis' in dim_data:
                lines.append(f"**åˆ†æ**:\n{dim_data['analysis']}\n\n")
            
            # ä¼˜åŠ¿
            if 'strengths' in dim_data and dim_data['strengths']:
                lines.append("**ä¼˜åŠ¿**:\n")
                if isinstance(dim_data['strengths'], list):
                    for strength in dim_data['strengths']:
                        lines.append(f"- {strength}\n")
                else:
                    lines.append(f"- {dim_data['strengths']}\n")
                lines.append("\n")
            
            # æ”¹è¿›å»ºè®®
            if 'improvements' in dim_data and dim_data['improvements']:
                lines.append("**æ”¹è¿›å»ºè®®**:\n")
                if isinstance(dim_data['improvements'], list):
                    for improvement in dim_data['improvements']:
                        lines.append(f"- {improvement}\n")
                else:
                    lines.append(f"- {dim_data['improvements']}\n")
                lines.append("\n")
            
            lines.append("---\n\n")
    
    # å¦‚æœæœ‰åŸå§‹å“åº”ä½†æ— æ³•è§£æ
    if 'raw_response' in analysis_result and not any(dim in analysis_result for dim in dimensions):
        lines.append("## ğŸ“„ åŸå§‹åˆ†æç»“æœ\n\n")
        lines.append(f"```\n{analysis_result['raw_response']}\n```\n")
    
    lines.append("\n---\n\n")
    lines.append("**æ³¨**: æœ¬æŠ¥å‘Šç”±AIè‡ªåŠ¨ç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚\n")
    
    return ''.join(lines)


def generate_daily_report(all_results, author_name, since_date=None, until_date=None):
    """
    ç”Ÿæˆå¼€å‘æ—¥æŠ¥æ ¼å¼çš„ Markdown æ–‡æ¡£
    
    Args:
        all_results: æŒ‰é¡¹ç›®åˆ†ç»„çš„æäº¤å­—å…¸
        author_name: æäº¤è€…å§“å
        since_date: èµ·å§‹æ—¥æœŸï¼ˆå¯é€‰ï¼‰
        until_date: ç»“æŸæ—¥æœŸï¼ˆå¯é€‰ï¼‰
    
    Returns:
        str: Markdown æ ¼å¼çš„æ—¥æŠ¥å†…å®¹
    """
    lines = []
    
    # ç¡®å®šæ—¥æœŸ
    if since_date and until_date and since_date == until_date:
        report_date = since_date
    else:
        report_date = datetime.now().strftime('%Y-%m-%d')
    
    date_obj = datetime.strptime(report_date, '%Y-%m-%d')
    date_formatted = date_obj.strftime('%Yå¹´%mæœˆ%dæ—¥')
    
    # æ ‡é¢˜
    lines.append(f"# {author_name} - å¼€å‘æ—¥æŠ¥\n")
    lines.append(f"**æ—¥æœŸ**: {date_formatted} ({report_date})\n")
    lines.append(f"**æäº¤è€…**: {author_name}\n")
    lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append("\n---\n\n")
    
    # å·¥ä½œæ¦‚è§ˆ
    total_projects = len(all_results)
    total_commits = sum(len(result['commits']) for result in all_results.values())
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    commit_types = defaultdict(int)
    commits_by_type = defaultdict(list)
    
    # æŒ‰é¡¹ç›®å’Œæ—¶é—´ç»„ç»‡æäº¤
    project_commits = {}
    time_range = {'start': None, 'end': None}
    
    for project_path, result in all_results.items():
        project = result['project']
        commits = result['commits']
        
        project_commits[project_path] = {
            'project': project,
            'commits': [],
            'types': defaultdict(int)
        }
        
        for commit in commits:
            commit_type, emoji = analyze_commit_type(commit.message)
            commit_types[commit_type] += 1
            commits_by_type[commit_type].append({
                'project': project_path,
                'commit': commit
            })
            
            # è§£ææ—¶é—´
            commit_time = commit.committed_date
            if isinstance(commit_time, str):
                time_obj = datetime.fromisoformat(commit_time.replace('Z', '+00:00'))
            else:
                time_obj = commit_time
            
            if time_range['start'] is None or time_obj < time_range['start']:
                time_range['start'] = time_obj
            if time_range['end'] is None or time_obj > time_range['end']:
                time_range['end'] = time_obj
            
            project_commits[project_path]['commits'].append({
                'commit': commit,
                'type': commit_type,
                'emoji': emoji,
                'time': time_obj
            })
            project_commits[project_path]['types'][commit_type] += 1
        
        # æŒ‰æ—¶é—´æ’åº
        project_commits[project_path]['commits'].sort(key=lambda x: x['time'], reverse=True)
    
    # å·¥ä½œæ¦‚è§ˆ
    lines.append("## ğŸ“Š å·¥ä½œæ¦‚è§ˆ\n\n")
    lines.append(f"- **æ¶‰åŠé¡¹ç›®**: {total_projects} ä¸ª\n")
    lines.append(f"- **æ€»æäº¤æ•°**: {total_commits} æ¬¡\n")
    
    if time_range['start'] and time_range['end']:
        start_str = time_range['start'].strftime('%H:%M')
        end_str = time_range['end'].strftime('%H:%M')
        lines.append(f"- **å·¥ä½œæ—¶é—´**: {start_str} - {end_str}\n")
    
    lines.append(f"- **å·¥ä½œç±»å‹åˆ†å¸ƒ**:\n")
    for commit_type, count in sorted(commit_types.items(), key=lambda x: x[1], reverse=True):
        emoji = analyze_commit_type('')[1] if commit_type == 'å…¶ä»–' else commits_by_type[commit_type][0]['commit'].message
        type_emoji = analyze_commit_type(commits_by_type[commit_type][0]['commit'].message)[1] if commits_by_type[commit_type] else 'ğŸ“Œ'
        lines.append(f"  - {type_emoji} {commit_type}: {count} æ¬¡\n")
    
    lines.append("\n---\n\n")
    
    # æŒ‰é¡¹ç›®è¯¦ç»†å·¥ä½œå†…å®¹
    lines.append("## ğŸ“¦ å·¥ä½œè¯¦æƒ…\n\n")
    
    for project_path in sorted(project_commits.keys()):
        project_info = project_commits[project_path]
        project = project_info['project']
        commits = project_info['commits']
        
        lines.append(f"### {project.name} ({project_path})\n")
        lines.append(f"**é¡¹ç›®é“¾æ¥**: [{project.web_url}]({project.web_url})\n")
        lines.append(f"**æäº¤æ•°**: {len(commits)} æ¬¡\n")
        
        # å·¥ä½œç±»å‹ç»Ÿè®¡
        if project_info['types']:
            type_summary_parts = []
            for t, c in sorted(project_info['types'].items(), key=lambda x: x[1], reverse=True):
                # è·å–è¯¥ç±»å‹çš„ emoji
                type_emoji = analyze_commit_type('')[1]  # é»˜è®¤
                for item in commits:
                    if item['type'] == t:
                        type_emoji = item['emoji']
                        break
                type_summary_parts.append(f"{type_emoji} {t}: {c}æ¬¡")
            lines.append(f"**å·¥ä½œç±»å‹**: {', '.join(type_summary_parts)}\n")
        
        lines.append("\n**æäº¤è®°å½•**:\n\n")
        
        for idx, item in enumerate(commits, 1):
            commit = item['commit']
            commit_type = item['type']
            emoji = item['emoji']
            time_str = item['time'].strftime('%H:%M')
            
            # è·å–è¯¦ç»†commitä¿¡æ¯
            try:
                details = get_commit_details(project, commit)
                short_message = details['short_message']
                full_message = details['full_message']
                stats = details['stats']
                changed_files = details['changed_files']
            except Exception as e:
                logger.debug(f"è·å–commitè¯¦æƒ…å¤±è´¥: {str(e)}")
                short_message = commit.message.split('\n')[0] if commit.message else ''
                full_message = commit.message or ''
                stats = None
                changed_files = []
            
            commit_id = commit.id[:8]
            commit_url = getattr(commit, 'web_url', '')
            
            lines.append(f"{idx}. **{emoji} [{commit_type}]** [{commit_id}]({commit_url}) {short_message}\n")
            lines.append(f"   - æ—¶é—´: {time_str}\n")
            
            # æ˜¾ç¤ºå®Œæ•´çš„commit messageï¼ˆå¦‚æœæœ‰å¤šè¡Œï¼‰
            if full_message and '\n' in full_message:
                # ç¼©è¿›æ˜¾ç¤ºå®Œæ•´ä¿¡æ¯
                indented_message = '\n   '.join(full_message.split('\n'))
                lines.append(f"   - å®Œæ•´æäº¤ä¿¡æ¯:\n   ```\n   {indented_message}\n   ```\n")
            
            # æ˜¾ç¤ºä»£ç è¡Œæ•°ç»Ÿè®¡
            if stats:
                lines.append(f"   - ä»£ç å˜æ›´: +{stats.get('additions', 0)} -{stats.get('deletions', 0)} (æ€»è®¡: {stats.get('total', 0)} è¡Œ)\n")
            elif hasattr(commit, 'stats') and commit.stats:
                try:
                    commit_stats = commit.stats
                    if isinstance(commit_stats, dict):
                        lines.append(f"   - ä»£ç å˜æ›´: +{commit_stats.get('additions', 0)} -{commit_stats.get('deletions', 0)}\n")
                except:
                    pass
            
            # æ˜¾ç¤ºæ–‡ä»¶å˜æ›´åˆ—è¡¨ï¼ˆæœ€å¤šæ˜¾ç¤º3ä¸ªï¼‰
            if changed_files:
                lines.append(f"   - å˜æ›´æ–‡ä»¶ ({len(changed_files)} ä¸ª): ")
                file_paths = []
                for file_info in changed_files[:3]:
                    file_path = file_info.get('new_path') or file_info.get('old_path') or file_info.get('path', '')
                    if file_path:
                        file_paths.append(f"`{file_path}`")
                lines.append(', '.join(file_paths))
                if len(changed_files) > 3:
                    lines.append(f" ç­‰ {len(changed_files)} ä¸ªæ–‡ä»¶")
                lines.append("\n")
        
        lines.append("\n---\n\n")
    
    # å·¥ä½œåˆ†ç±»æ±‡æ€»
    lines.append("## ğŸ“‹ å·¥ä½œåˆ†ç±»æ±‡æ€»\n\n")
    
    for commit_type in sorted(commit_types.keys(), key=lambda x: commit_types[x], reverse=True):
        type_emoji = analyze_commit_type(commits_by_type[commit_type][0]['commit'].message)[1] if commits_by_type[commit_type] else 'ğŸ“Œ'
        lines.append(f"### {type_emoji} {commit_type} ({commit_types[commit_type]} æ¬¡)\n\n")
        
        # æŒ‰é¡¹ç›®åˆ†ç»„
        by_project = defaultdict(list)
        for item in commits_by_type[commit_type]:
            by_project[item['project']].append(item['commit'])
        
        for project_path in sorted(by_project.keys()):
            project = all_results[project_path]['project']
            commits = by_project[project_path]
            
            lines.append(f"**{project.name}** ({len(commits)} æ¬¡):\n")
            for commit in commits:
                commit_id = commit.id[:8]
                commit_message = commit.message.split('\n')[0]
                lines.append(f"- [{commit_id}]({commit.web_url}) {commit_message}\n")
            lines.append("\n")
        
        lines.append("---\n\n")
    
    # æ—¶é—´çº¿
    lines.append("## â° å·¥ä½œæ—¶é—´çº¿\n\n")
    
    all_commits_timeline = []
    for project_path, result in all_results.items():
        for commit in result['commits']:
            commit_time = commit.committed_date
            if isinstance(commit_time, str):
                time_obj = datetime.fromisoformat(commit_time.replace('Z', '+00:00'))
            else:
                time_obj = commit_time
            
            commit_type, emoji = analyze_commit_type(commit.message)
            all_commits_timeline.append({
                'time': time_obj,
                'project': project_path,
                'commit': commit,
                'type': commit_type,
                'emoji': emoji
            })
    
    all_commits_timeline.sort(key=lambda x: x['time'], reverse=True)
    
    for item in all_commits_timeline:
        time_str = item['time'].strftime('%H:%M')
        commit = item['commit']
        commit_id = commit.id[:8]
        commit_message = commit.message.split('\n')[0]
        
        lines.append(f"- **{time_str}** {item['emoji']} [{item['project']}]({all_results[item['project']]['project'].web_url}) - [{commit_id}]({commit.web_url}) {commit_message}\n")
    
    lines.append("\n---\n\n")
    
    # æ€»ç»“
    lines.append("## ğŸ“ å·¥ä½œæ€»ç»“\n\n")
    lines.append(f"ä»Šæ—¥å…±å®Œæˆ {total_commits} æ¬¡æäº¤ï¼Œæ¶‰åŠ {total_projects} ä¸ªé¡¹ç›®ã€‚")
    
    if commit_types:
        main_work = max(commit_types.items(), key=lambda x: x[1])
        lines.append(f"ä¸»è¦å·¥ä½œç±»å‹ä¸º **{main_work[0]}**ï¼ˆ{main_work[1]} æ¬¡ï¼‰ã€‚")
    
    lines.append("\n")
    lines.append("\n---\n\n")
    
    # æ·»åŠ ä»£ç ç»Ÿè®¡å’Œè¯„åˆ†ä¿¡æ¯
    try:
        code_stats = calculate_code_statistics(all_results, since_date, until_date)
        scores = calculate_scores(all_results, since_date, until_date)
        
        lines.append("## ğŸ“Š ä»£ç ç»Ÿè®¡\n\n")
        if code_stats['commits_with_stats'] > 0:
            lines.append(f"- **æ€»æ–°å¢è¡Œæ•°**: {code_stats['total_additions']:,}\n")
            lines.append(f"- **æ€»åˆ é™¤è¡Œæ•°**: {code_stats['total_deletions']:,}\n")
            lines.append(f"- **å‡€å¢è¡Œæ•°**: {code_stats['net_lines']:,}\n")
            lines.append(f"- **å¹³å‡æ¯æ¬¡æäº¤ä»£ç è¡Œæ•°**: {code_stats['avg_lines_per_commit']}\n")
        else:
            lines.append("- **ä»£ç è¡Œæ•°ç»Ÿè®¡**: æš‚ä¸å¯ç”¨ï¼ˆéœ€è¦APIæƒé™ï¼‰\n")
        lines.append("\n---\n\n")
        
        lines.append("## ğŸ¯ å¤šç»´åº¦è¯„åˆ†\n\n")
        lines.append(f"**æ€»ä½“è¯„åˆ†**: {scores['overall']:.2f} / 100\n\n")
        
        def progress_bar(score, max_score=100, length=15):
            """ç”Ÿæˆè¿›åº¦æ¡"""
            filled = int(score / max_score * length)
            bar = 'â–ˆ' * filled + 'â–‘' * (length - filled)
            return f"`{bar}` {score:.1f}%"
        
        lines.append(f"- **å‹¤å¥‹åº¦**: {progress_bar(scores['diligence']['score'])} (æ´»è·ƒ {scores['diligence']['active_days']} å¤©ï¼Œå¹³å‡ {scores['diligence']['frequency']:.2f} æ¬¡/å¤©)\n")
        lines.append(f"- **ç¨³å®šæ€§**: {progress_bar(scores['stability']['score'])} ({len(scores['stability']['monthly_commits'])} ä¸ªæœˆæœ‰æäº¤)\n")
        lines.append(f"- **è§£å†³é—®é¢˜èƒ½åŠ›**: {progress_bar(scores['problem_solving']['score'])} (ä¿®å¤ç±»æäº¤å æ¯” {scores['problem_solving']['ratio']:.1%})\n")
        lines.append(f"- **åŠŸèƒ½åˆ›æ–°åŠ›**: {progress_bar(scores['feature_innovation']['score'])} (åŠŸèƒ½ç±»æäº¤å æ¯” {scores['feature_innovation']['ratio']:.1%})\n")
        lines.append(f"- **å¤šçº¿ä½œæˆ˜èƒ½åŠ›**: {progress_bar(scores['versatility']['score'])} ({scores['versatility']['project_count']} ä¸ªé¡¹ç›®ï¼Œè·¨åº¦ {scores['versatility']['time_span_days']} å¤©)\n")
        
        lines.append("\n")
    except Exception as e:
        logger.warning(f"ç”Ÿæˆç»Ÿè®¡å’Œè¯„åˆ†ä¿¡æ¯æ—¶å‡ºé”™: {str(e)}")
        lines.append("## ğŸ“Š ä»£ç ç»Ÿè®¡\n\n")
        lines.append("- **ä»£ç ç»Ÿè®¡**: ç”Ÿæˆæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥æ•°æ®\n\n")
    
    return ''.join(lines)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä» GitLab ä»“åº“è·å–æäº¤è€…æ¯å¤©çš„ä»£ç æäº¤ï¼Œç”Ÿæˆ Markdown æ ¼å¼æ—¥å¿—',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åŸºæœ¬ç”¨æ³•ï¼šæŒ‡å®šä»“åº“ã€åˆ†æ”¯ã€æäº¤è€…ï¼ˆè¾“å‡ºæ–‡ä»¶åè‡ªåŠ¨ä½¿ç”¨å½“å¤©æ—¥æœŸï¼‰
  python git2logs.py --repo http://gitlab.example.com/group/project.git --branch main --author "MIZUKI" --token YOUR_TOKEN
  
  # è·å–ä»Šå¤©çš„æäº¤ï¼ˆè‡ªåŠ¨ä½¿ç”¨ä»Šå¤©æ—¥æœŸä½œä¸ºæ–‡ä»¶åå‰ç¼€ï¼‰
  python git2logs.py --repo http://gitlab.example.com/group/project.git --branch develop --author "MIZUKI" --today --token YOUR_TOKEN
  
  # è‡ªåŠ¨æ‰«ææ‰€æœ‰é¡¹ç›®ï¼ŒæŸ¥æ‰¾æŒ‡å®šæäº¤è€…ä»Šå¤©çš„æäº¤
  python git2logs.py --scan-all --gitlab-url http://gitlab.example.com --author "MIZUKI" --today --token YOUR_TOKEN
  
  # è‡ªåŠ¨æ‰«ææ‰€æœ‰é¡¹ç›®ï¼ŒæŒ‡å®šåˆ†æ”¯å’Œæ—¥æœŸèŒƒå›´
  python git2logs.py --scan-all --gitlab-url http://gitlab.example.com --branch master --author "MIZUKI" --since 2024-01-01 --until 2024-12-31 --token YOUR_TOKEN
  
  # æ‰‹åŠ¨æŒ‡å®šè¾“å‡ºæ–‡ä»¶
  python git2logs.py --repo group/project --branch main --author "John Doe" --output commits.md
        """
    )
    
    parser.add_argument(
        '--repo',
        help='GitLab ä»“åº“åœ°å€æˆ–è·¯å¾„ï¼ˆä¾‹å¦‚ï¼šhttps://gitlab.com/group/project æˆ– group/projectï¼‰ã€‚å¦‚æœä½¿ç”¨ --scan-allï¼Œåˆ™ä¸éœ€è¦æ­¤å‚æ•°'
    )
    
    parser.add_argument(
        '--author',
        required=True,
        help='æäº¤è€…å§“åæˆ–é‚®ç®±'
    )
    
    parser.add_argument(
        '--scan-all',
        action='store_true',
        help='è‡ªåŠ¨æ‰«ææ‰€æœ‰æœ‰æƒé™è®¿é—®çš„é¡¹ç›®ï¼ŒæŸ¥æ‰¾æŒ‡å®šæäº¤è€…çš„æäº¤ï¼ˆéœ€è¦æä¾› GitLab URL å’Œè®¿é—®ä»¤ç‰Œï¼‰'
    )
    
    parser.add_argument(
        '--token',
        help='GitLab è®¿é—®ä»¤ç‰Œï¼ˆç§æœ‰ä»“åº“éœ€è¦ï¼Œä¹Ÿå¯é€šè¿‡ç¯å¢ƒå˜é‡ GITLAB_TOKEN è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--gitlab-url',
        default='https://gitlab.com',
        help='GitLab å®ä¾‹ URLï¼ˆé»˜è®¤ï¼šhttps://gitlab.comï¼‰'
    )
    
    parser.add_argument(
        '--since',
        help='èµ·å§‹æ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰'
    )
    
    parser.add_argument(
        '--until',
        help='ç»“æŸæ—¥æœŸï¼ˆæ ¼å¼ï¼šYYYY-MM-DDï¼‰'
    )
    
    parser.add_argument(
        '--branch',
        help='æŒ‡å®šåˆ†æ”¯åç§°ï¼ˆé»˜è®¤æŸ¥è¯¢æ‰€æœ‰åˆ†æ”¯ï¼‰'
    )
    
    parser.add_argument(
        '--today',
        action='store_true',
        help='ä»…è·å–ä»Šå¤©çš„æäº¤ï¼ˆè‡ªåŠ¨è®¾ç½®æ—¥æœŸèŒƒå›´ä¸ºä»Šå¤©ï¼‰'
    )
    
    parser.add_argument(
        '--output',
        '-o',
        help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šä½¿ç”¨å½“å¤©æ—¥æœŸä½œä¸ºæ–‡ä»¶åå‰ç¼€ï¼Œæ ¼å¼ï¼šYYYY-MM-DD_commits.mdï¼‰'
    )
    
    parser.add_argument(
        '--daily-report',
        action='store_true',
        help='ç”Ÿæˆå¼€å‘æ—¥æŠ¥æ ¼å¼ï¼ˆæ›´è¯¦ç»†çš„å·¥ä½œåˆ†æå’Œåˆ†ç±»ï¼‰'
    )
    
    parser.add_argument(
        '--statistics',
        action='store_true',
        help='ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šæ ¼å¼ï¼ˆåŒ…å«ä»£ç è¡Œæ•°ç»Ÿè®¡å’Œå¤šç»´åº¦è¯„åˆ†ï¼‰'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.scan_all and args.repo:
        logger.error("--scan-all å’Œ --repo ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        sys.exit(1)
    
    if not args.scan_all and not args.repo:
        logger.error("å¿…é¡»æä¾› --repo æˆ–ä½¿ç”¨ --scan-all")
        sys.exit(1)
    
    # å¦‚æœæŒ‡å®šäº† --todayï¼Œè‡ªåŠ¨è®¾ç½®æ—¥æœŸèŒƒå›´ä¸ºä»Šå¤©
    if args.today:
        today = datetime.now().strftime('%Y-%m-%d')
        args.since = today
        args.until = today
        logger.info(f"å·²è®¾ç½®æ—¥æœŸèŒƒå›´ä¸ºä»Šå¤©: {today}")
    
    try:
        # è·å–è®¿é—®ä»¤ç‰Œï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå…¶æ¬¡ä½¿ç”¨ç¯å¢ƒå˜é‡ï¼‰
        token = args.token or None
        if not token:
            token = os.environ.get('GITLAB_TOKEN')
        
        if not token:
            logger.error("å¿…é¡»æä¾›è®¿é—®ä»¤ç‰Œï¼ˆ--token æˆ–ç¯å¢ƒå˜é‡ GITLAB_TOKENï¼‰")
            sys.exit(1)
        
        # ç¡®å®š GitLab URL
        gitlab_url = args.gitlab_url
        
        if args.scan_all:
            # æ‰«ææ‰€æœ‰é¡¹ç›®æ¨¡å¼
            if not gitlab_url or gitlab_url == 'https://gitlab.com':
                logger.error("ä½¿ç”¨ --scan-all æ—¶å¿…é¡»æŒ‡å®š --gitlab-url")
                sys.exit(1)
            
            logger.info(f"ä½¿ç”¨è‡ªåŠ¨æ‰«ææ¨¡å¼ï¼ŒGitLab å®ä¾‹: {gitlab_url}")
            
            # åˆ›å»º GitLab å®¢æˆ·ç«¯
            gl = create_gitlab_client(gitlab_url, token)
            
            # æ‰«ææ‰€æœ‰é¡¹ç›®
            all_results = scan_all_projects(
                gl,
                args.author,
                since_date=args.since,
                until_date=args.until,
                branch=args.branch
            )
            
            if not all_results:
                logger.warning(f"æœªåœ¨ä»»ä½•é¡¹ç›®ä¸­æ‰¾åˆ°æäº¤è€… '{args.author}' çš„æäº¤è®°å½•")
                sys.exit(0)
            
            # ç”Ÿæˆ Markdown æ—¥å¿—
            if args.statistics:
                markdown_content = generate_statistics_report(
                    all_results,
                    args.author,
                    since_date=args.since,
                    until_date=args.until
                )
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
                if args.output:
                    output_file = args.output
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼Œå¦‚æœæ˜¯ç›®å½•åˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                    if os.path.isdir(output_file):
                        today = datetime.now().strftime('%Y-%m-%d')
                        branch_suffix = f"_{args.branch}" if args.branch else ""
                        filename = f"{today}_statistics{branch_suffix}.md"
                        output_file = os.path.join(output_file, filename)
                        logger.info(f"è¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å: {output_file}")
                    # å¦‚æœè¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md
                    elif not os.path.splitext(output_file)[1]:
                        output_file = output_file + '.md'
                        logger.info(f"è¾“å‡ºæ–‡ä»¶æ— æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md: {output_file}")
                else:
                    today = datetime.now().strftime('%Y-%m-%d')
                    branch_suffix = f"_{args.branch}" if args.branch else ""
                    output_file = f"{today}_statistics{branch_suffix}.md"
            elif args.daily_report:
                markdown_content = generate_daily_report(
                    all_results,
                    args.author,
                    since_date=args.since,
                    until_date=args.until
                )
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
                if args.output:
                    output_file = args.output
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼Œå¦‚æœæ˜¯ç›®å½•åˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                    if os.path.isdir(output_file):
                        today = datetime.now().strftime('%Y-%m-%d')
                        branch_suffix = f"_{args.branch}" if args.branch else ""
                        filename = f"{today}_daily_report{branch_suffix}.md"
                        output_file = os.path.join(output_file, filename)
                        logger.info(f"è¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å: {output_file}")
                    # å¦‚æœè¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md
                    elif not os.path.splitext(output_file)[1]:
                        output_file = output_file + '.md'
                        logger.info(f"è¾“å‡ºæ–‡ä»¶æ— æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md: {output_file}")
                else:
                    today = datetime.now().strftime('%Y-%m-%d')
                    branch_suffix = f"_{args.branch}" if args.branch else ""
                    output_file = f"{today}_daily_report{branch_suffix}.md"
            else:
                markdown_content = generate_multi_project_markdown(
                    all_results,
                    args.author,
                    since_date=args.since,
                    until_date=args.until
                )
                # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
                if args.output:
                    output_file = args.output
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼Œå¦‚æœæ˜¯ç›®å½•åˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                    if os.path.isdir(output_file):
                        today = datetime.now().strftime('%Y-%m-%d')
                        branch_suffix = f"_{args.branch}" if args.branch else ""
                        filename = f"{today}_all_projects{branch_suffix}.md"
                        output_file = os.path.join(output_file, filename)
                        logger.info(f"è¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å: {output_file}")
                    # å¦‚æœè¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md
                    elif not os.path.splitext(output_file)[1]:
                        output_file = output_file + '.md'
                        logger.info(f"è¾“å‡ºæ–‡ä»¶æ— æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md: {output_file}")
                else:
                    today = datetime.now().strftime('%Y-%m-%d')
                    branch_suffix = f"_{args.branch}" if args.branch else ""
                    output_file = f"{today}_all_projects{branch_suffix}.md"
        
        else:
            # å•é¡¹ç›®æ¨¡å¼
            # å¦‚æœä»“åº“ URL æ˜¯å®Œæ•´ URLï¼Œå°è¯•ä»ä¸­æå– GitLab URL
            extracted_url = extract_gitlab_url(args.repo)
            if extracted_url:
                gitlab_url = extracted_url
                logger.info(f"ä»ä»“åº“ URL æå– GitLab å®ä¾‹: {gitlab_url}")
            
            # åˆ›å»º GitLab å®¢æˆ·ç«¯
            gl = create_gitlab_client(gitlab_url, token)
            
            # è§£æé¡¹ç›®æ ‡è¯†ç¬¦
            project_id = parse_project_identifier(args.repo)
            logger.info(f"é¡¹ç›®æ ‡è¯†ç¬¦: {project_id}")
            
            # è·å–é¡¹ç›®
            try:
                project = gl.projects.get(project_id)
                logger.info(f"æˆåŠŸè·å–é¡¹ç›®: {project.name}")
            except Exception as e:
                logger.error(f"è·å–é¡¹ç›®å¤±è´¥: {str(e)}")
                logger.error("è¯·æ£€æŸ¥é¡¹ç›®è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼Œä»¥åŠæ˜¯å¦æœ‰è®¿é—®æƒé™")
                sys.exit(1)
            
            # è·å–æäº¤è®°å½•
            commits = get_commits_by_author(
                project,
                args.author,
                since_date=args.since,
                until_date=args.until,
                branch=args.branch
            )
            
            if not commits:
                logger.warning(f"æœªæ‰¾åˆ°æäº¤è€… '{args.author}' çš„æäº¤è®°å½•")
                sys.exit(0)
            
            # æŒ‰æ—¥æœŸåˆ†ç»„
            grouped_commits = group_commits_by_date(commits)
            
            # ç”Ÿæˆ Markdown æ—¥å¿—
            markdown_content = generate_markdown_log(
                grouped_commits,
                args.author,
                repo_name=project.name,
                project=project
            )
            
            # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
            if args.output:
                output_file = args.output
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›®å½•ï¼Œå¦‚æœæ˜¯ç›®å½•åˆ™è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                if os.path.isdir(output_file):
                    today = datetime.now().strftime('%Y-%m-%d')
                    branch_suffix = f"_{args.branch}" if args.branch else ""
                    if args.daily_report:
                        filename = f"{today}_daily_report{branch_suffix}.md"
                    else:
                        filename = f"{today}_all_projects{branch_suffix}.md"
                    output_file = os.path.join(output_file, filename)
                    logger.info(f"è¾“å‡ºè·¯å¾„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å: {output_file}")
                # å¦‚æœè¾“å‡ºæ–‡ä»¶æ²¡æœ‰æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md
                elif not os.path.splitext(output_file)[1]:
                    output_file = output_file + '.md'
                    logger.info(f"è¾“å‡ºæ–‡ä»¶æ— æ‰©å±•åï¼Œè‡ªåŠ¨æ·»åŠ  .md: {output_file}")
            else:
                # å¦‚æœæœªæŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œä½¿ç”¨å½“å¤©æ—¥æœŸä½œä¸ºæ–‡ä»¶åå‰ç¼€
                today = datetime.now().strftime('%Y-%m-%d')
                branch_suffix = f"_{args.branch}" if args.branch else ""
                output_file = f"{today}_commits{branch_suffix}.md"
        
        # è¾“å‡ºç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        logger.info(f"æ—¥å¿—å·²ä¿å­˜åˆ°: {output_file}")
    
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()
